{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## The preamble handles a few imports.\n",
       "## It also loads common functions and dicts: \n",
       "\n",
       "## Bosch challenge specific:\n",
       "* **getMCC**(tp,tn,fp,fn)\n",
       "\n",
       "## ML/Analytics functions:\n",
       "* **compare_train_test**(clf, ds_train, label_train, ds_test, label_test, mva='MVA', bins=50, use_vote=None, log=False)\n",
       "* **plot_classifier_output**( pred_train, pred_test, y_train, y_test, multipagepdf=None, bins = None, normalised = True )\n",
       "* **plot_correlations**(data,label='', \\*\\*kwds)\n",
       "* **optimisePars**(mva, points, data , classes, fraction=0.7, score = 'log_loss', cvs=5)\n",
       "\n",
       "---\n",
       "\n",
       "## Various\n",
       "* **showUniques**(df)\n",
       "* **ensure_dir**(directory)\n",
       "* **printBumper**(text, c='=', n=-1)\n",
       "* **intersec**(d1, d2)\n",
       "* **union**(d1, d2)\n",
       "\n",
       "---\n",
       "\n",
       "## Color dictionaries:\n",
       "* **Tableau10**\n",
       "* **Tableau10_Light**\n",
       "* **Tableau10_Medium**\n",
       "* **Tableau_20**\n",
       "* **ColorBlind10**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run preamble.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "\n",
    "## Group data into stations\n",
    "In this way we have logical packages of data w/o too many NaNs.\n",
    "\n",
    "## Unsupervised dimensionalty reduction using PCA\n",
    "This should result in better and most importantly less variables.\n",
    "Then choose best six with an ANOVA filter.\n",
    "\n",
    "## Supervised predictions using a BDT \n",
    "Use BDT on 6 (or less) variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"files/train_numeric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addLineStation(ds):\n",
    "    \"\"\"\n",
    "    Creates a MultiIndex object with levels:\n",
    "    Line, Station, Feature.\n",
    "    \n",
    "    Arguments:\n",
    "    ds - (DataFrame) Dataset from which to extract the MultiIndex\n",
    "    \"\"\"\n",
    "    l0=[];l1=[];l2=[]\n",
    "    for c in ds:\n",
    "        s = c.split(sep=\"_\")\n",
    "        l0.append(s[0])\n",
    "        l1.append(s[0]+\"_\"+s[1])\n",
    "        l2.append(c)\n",
    "    return pd.MultiIndex.from_arrays([l0,l1,l2],names=[\"Line\",\"Station\",\"Feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a little bit of index magic to be able to group by station\n",
    "train = train.set_index([\"Id\",\"Response\"])\n",
    "train.columns = addLineStation(train)\n",
    "\n",
    "gb = train.groupby(level=\"Station\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindrop = [\"Id\",\"Response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainMVA(grps, processed=[], save=False):\n",
    "    mvas = {}\n",
    "    for i,g in enumerate(grps.groups):\n",
    "        s=time.time()\n",
    "        if g in processed:\n",
    "            print(g, \" already trained. Continue.\")\n",
    "            continue\n",
    "        if i%10==0 and i>0 and save:\n",
    "            print(\"pickle intermediate results at i=\",i)\n",
    "            joblib.dump(mvas,\"files/mvas_per_station{}.pkl\".format(i)\n",
    "                        , compress = 3)\n",
    "            mvas={}\n",
    "        # Dataset magic\n",
    "        d = gb.get_group(g).dropna(how=\"all\", axis=0)\n",
    "        d.columns = d.columns.droplevel([\"Line\",\"Station\"])\n",
    "        d=d.reset_index()\n",
    "    \n",
    "        n_features = len(d.columns)-2\n",
    "        \n",
    "        n_pca = 10 if n_features>10 else n_features\n",
    "        n_annova = 6 if n_pca>6 else n_pca\n",
    "        n_max_feat_bdt = 4 if n_annova==6 else n_annova\n",
    "    \n",
    "        # filter\n",
    "        # feature selection\n",
    "        anova_filter = SelectKBest(f_classif, k=n_annova)\n",
    "    \n",
    "        # classifier\n",
    "        bdt = AdaBoostClassifier(DecisionTreeClassifier(max_features=n_max_feat_bdt\n",
    "                                                        , max_depth=3)\n",
    "                        , n_estimators = 600\n",
    "                        , learning_rate = 0.01)\n",
    "        # pipeline\n",
    "        print(\"classifier pipeline fit for goup: \"\n",
    "              , g, \"(\",i+1,\"/\",len(gb.groups),\")\")\n",
    "       \n",
    "        clf = Pipeline([ (\"fillVals\", Imputer(strategy=\"mean\",verbose=1))\n",
    "                       , (\"dimReduction\", PCA(n_components=n_pca))\n",
    "                       , ('feature_selection', anova_filter)\n",
    "                       , ('mva', bdt)])\n",
    "    \n",
    "        clf.fit(d.drop(traindrop,axis=1), d.Response)\n",
    "        print(\"done in {:2.2f}\".format((time.time()-s)/60.),\" minutes.\")\n",
    "        mvas[g] = clf\n",
    "        del d\n",
    "    joblib.dump(mvas,\"files/mvas_per_station{}.pkl\".format(len(grps.groups)+2)\n",
    "                , compress = 3)        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1_S24  already trained. Continue.\n",
      "L0_S14  already trained. Continue.\n",
      "L2_S26  already trained. Continue.\n",
      "L3_S39  already trained. Continue.\n",
      "L3_S33  already trained. Continue.\n",
      "L3_S31  already trained. Continue.\n",
      "L0_S4  already trained. Continue.\n",
      "L3_S30  already trained. Continue.\n",
      "L3_S38  already trained. Continue.\n",
      "L3_S29  already trained. Continue.\n",
      "L0_S9  already trained. Continue.\n",
      "L2_S27  already trained. Continue.\n",
      "L0_S21  already trained. Continue.\n",
      "L3_S43  already trained. Continue.\n",
      "L3_S49  already trained. Continue.\n",
      "L0_S17  already trained. Continue.\n",
      "L0_S19  already trained. Continue.\n",
      "L3_S47  already trained. Continue.\n",
      "L3_S45  already trained. Continue.\n",
      "L0_S23  already trained. Continue.\n",
      "L0_S6  already trained. Continue.\n",
      "L3_S40  already trained. Continue.\n",
      "L0_S18  already trained. Continue.\n",
      "L3_S34  already trained. Continue.\n",
      "L0_S20  already trained. Continue.\n",
      "L0_S8  already trained. Continue.\n",
      "L0_S0  already trained. Continue.\n",
      "L0_S10  already trained. Continue.\n",
      "L0_S2  already trained. Continue.\n",
      "L0_S11  already trained. Continue.\n",
      "L3_S36  already trained. Continue.\n",
      "L3_S37  already trained. Continue.\n",
      "L3_S51  already trained. Continue.\n",
      "L0_S22  already trained. Continue.\n",
      "L1_S25  already trained. Continue.\n",
      "L3_S35  already trained. Continue.\n",
      "L0_S16  already trained. Continue.\n",
      "L0_S13  already trained. Continue.\n",
      "L0_S5  already trained. Continue.\n",
      "L0_S3  already trained. Continue.\n",
      "L3_S44  already trained. Continue.\n",
      "L3_S50  already trained. Continue.\n",
      "L2_S28  already trained. Continue.\n",
      "L0_S7  already trained. Continue.\n",
      "L0_S12  already trained. Continue.\n",
      "L3_S32  already trained. Continue.\n",
      "L3_S41  already trained. Continue.\n",
      "L3_S48  already trained. Continue.\n",
      "L0_S15  already trained. Continue.\n",
      "L0_S1  already trained. Continue.\n",
      "Wall time: 1.5 ms\n"
     ]
    }
   ],
   "source": [
    "# do all the heavy lifting!\n",
    "%time trainMVA(gb,trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained = joblib.load(\"files/mvas_per_station.pkl\")\n",
    "trained.update(joblib.load(\"files/mvas_per_station10.pkl\"))\n",
    "trained.update(joblib.load(\"files/mvas_per_station20.pkl\"))\n",
    "trained.update(joblib.load(\"files/mvas_per_station30.pkl\"))\n",
    "trained.update(joblib.load(\"files/mvas_per_station40.pkl\"))\n",
    "trained.update(joblib.load(\"files/mvas_per_station50.pkl\"))\n",
    "trained.update(joblib.load(\"files/mvas_per_station51.pkl\"))\n",
    "trained.update(joblib.load(\"files/mvas_per_station52.pkl\"))\n",
    "\n",
    "# run again to get all (small coding bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictStations(grps, mvas, results, processed=[], save=False):\n",
    "    for i,g in enumerate(grps.groups):\n",
    "        s=time.time()\n",
    "        if g in processed:\n",
    "            print(g, \" already predicted. Continue.\")\n",
    "            continue\n",
    "        # Dataset magic\n",
    "        d = grps.get_group(g).dropna(how=\"all\", axis=0)\n",
    "        d.columns = d.columns.droplevel([\"Line\",\"Station\"])\n",
    "        d=d.reset_index()\n",
    "        if len(d)==0:\n",
    "            continue\n",
    "        # pipeline\n",
    "        print(\"classifier pipeline prediction for goup: \"\n",
    "              , g, \"(\",i+1,\"/\",len(grps.groups),\")\")\n",
    "        clf = mvas[g]\n",
    "    \n",
    "        pred_func = pd.DataFrame({\"Id\" : d.Id\n",
    "                            ,\"pred_\"+g :clf.decision_function(d.drop(traindrop,axis=1))})\n",
    "        results = results.merge(pred_func, on=\"Id\", how=\"left\")\n",
    "        print(\"done in {:2.2f}\".format((time.time()-s)/60.),\" minutes.\")\n",
    "        del d\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"Id\": train.reset_index()[\"Id\"],\n",
    "                       \"Response\": train.reset_index()[\"Response\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier pipeline prediction for goup:  L1_S24 ( 1 / 50 )\n",
      "done in 0.59  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S14 ( 2 / 50 )\n",
      "done in 0.39  minutes.\n",
      "classifier pipeline prediction for goup:  L2_S26 ( 3 / 50 )\n",
      "done in 0.64  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S39 ( 4 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S33 ( 5 / 50 )\n",
      "done in 2.85  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S31 ( 6 / 50 )\n",
      "done in 0.10  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S4 ( 7 / 50 )\n",
      "done in 0.84  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S30 ( 8 / 50 )\n",
      "done in 2.93  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S38 ( 9 / 50 )\n",
      "done in 0.07  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S29 ( 10 / 50 )\n",
      "done in 2.82  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S9 ( 11 / 50 )\n",
      "done in 0.55  minutes.\n",
      "classifier pipeline prediction for goup:  L2_S27 ( 12 / 50 )\n",
      "done in 0.32  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S21 ( 13 / 50 )\n",
      "done in 0.21  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S43 ( 14 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S49 ( 15 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S17 ( 16 / 50 )\n",
      "done in 0.32  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S19 ( 17 / 50 )\n",
      "done in 0.31  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S47 ( 18 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S45 ( 19 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S23 ( 20 / 50 )\n",
      "done in 0.21  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S6 ( 21 / 50 )\n",
      "done in 0.82  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S40 ( 22 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S18 ( 23 / 50 )\n",
      "done in 0.30  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S34 ( 24 / 50 )\n",
      "done in 2.59  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S20 ( 25 / 50 )\n",
      "done in 0.57  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S8 ( 26 / 50 )\n",
      "done in 1.60  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S0 ( 27 / 50 )\n",
      "done in 1.65  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S10 ( 28 / 50 )\n",
      "done in 0.56  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S2 ( 29 / 50 )\n",
      "done in 0.84  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S11 ( 30 / 50 )\n",
      "done in 0.56  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S36 ( 31 / 50 )\n",
      "done in 1.37  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S37 ( 32 / 50 )\n",
      "done in 2.65  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S51 ( 33 / 50 )\n",
      "done in 0.21  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S22 ( 34 / 50 )\n",
      "done in 0.22  minutes.\n",
      "classifier pipeline prediction for goup:  L1_S25 ( 35 / 50 )\n",
      "done in 0.36  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S35 ( 36 / 50 )\n",
      "done in 1.37  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S16 ( 37 / 50 )\n",
      "done in 0.31  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S13 ( 38 / 50 )\n",
      "done in 0.57  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S5 ( 39 / 50 )\n",
      "done in 0.81  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S3 ( 40 / 50 )\n",
      "done in 0.84  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S44 ( 41 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S50 ( 42 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L2_S28 ( 43 / 50 )\n",
      "done in 0.05  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S7 ( 44 / 50 )\n",
      "done in 0.81  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S12 ( 45 / 50 )\n",
      "done in 0.61  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S32 ( 46 / 50 )\n",
      "done in 0.07  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S41 ( 47 / 50 )\n",
      "done in 0.18  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S48 ( 48 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S15 ( 49 / 50 )\n",
      "done in 0.32  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S1 ( 50 / 50 )\n",
      "done in 1.59  minutes.\n"
     ]
    }
   ],
   "source": [
    "pre_functs = predictStations(gb, trained, result,pre_functs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotPreds(df):\n",
    "    for i,c in enumerate(df.columns.values):\n",
    "        if c in [\"Id\",\"Response\"]:\n",
    "            continue\n",
    "        a = sns.distplot(df[df.Response==0][c].dropna(),norm_hist=True,\n",
    "                label=\"good parts\", color=Tableau10_Medium['green'])\n",
    "        a = sns.distplot(df[df.Response==1][c].dropna(),norm_hist=True\n",
    "                , label=\"bad parts\", color=Tableau10_Medium['red'])\n",
    "        a.legend(loc=0)\n",
    "        a.figure.savefig(\"plots/PredictionFunctions/predFunc\"+c+\".png\")\n",
    "        plt.clf()\n",
    "        if i%10==9:\n",
    "            print(i+1,\"/\",len(df.columns.values)-2, \" plots made.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\statsmodels\\nonparametric\\kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 / 50  plots made.\n",
      "20 / 50  plots made.\n",
      "30 / 50  plots made.\n",
      "40 / 50  plots made.\n",
      "50 / 50  plots made.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1539c97c278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotPreds(pre_functs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optPrediction(df):\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    opt = []\n",
    "    for i,c in enumerate(df.columns.values):\n",
    "        if c in [\"Id\",\"Response\"]:\n",
    "            continue\n",
    "        subset = df[[\"Id\",\"Response\",c]].dropna()\n",
    "        # find threshold for prediction, use 50 steps\n",
    "        thresholds = np.linspace(subset[c].min(),subset[c].max())\n",
    "        tmp = -1\n",
    "        best = -1\n",
    "        best_z = subset[c].min()\n",
    "        for z in thresholds:\n",
    "            tmp = matthews_corrcoef(subset.Response,\n",
    "                                    [1 if x>z else 0 for x in subset[c]])\n",
    "            if tmp>best:\n",
    "                best=tmp\n",
    "                best_z = z\n",
    "        opt.append([c,best_z,best])\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 56s\n"
     ]
    }
   ],
   "source": [
    "%time thresholds_mcc_per_station = optPrediction(pre_functs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = pd.DataFrame({\n",
    "        \"Station\" : [x[0] for x in thresholds_mcc_per_station],\n",
    "        \"Threshold\" : [x[1] for x in thresholds_mcc_per_station],\n",
    "        \"MCC\" : [x[2] for x in thresholds_mcc_per_station]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/station_mcc_threshold.pkl']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(stations, \"files/station_mcc_threshold.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/train_pred_functs.pkl',\n",
       " 'files/train_pred_functs.pkl_01.npy.z',\n",
       " 'files/train_pred_functs.pkl_02.npy.z']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pre_functs, \"files/train_pred_functs.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_functs = joblib.load(\"files/train_pred_functs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = joblib.load(\"files/station_mcc_threshold.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictId(pred_f,thresholds, pred_test=False):\n",
    "    \"\"\"\n",
    "    Strategy per Id will be:\n",
    "    Sort stations classifiers by descending MCC values.\n",
    "    Then predict Ids once with the best classifier available.\n",
    "    \"\"\"\n",
    "    sorted_stations = thresholds.sort_values(by=[\"MCC\"]\n",
    "                                             , ascending=False)\n",
    "    \n",
    "    results = pred_f[[\"Id\",\"Response\"]] if pred_test else pred_f[[\"Id\"]]\n",
    "    results[\"pred_Response\"] = 0\n",
    "    \n",
    "    for s in sorted_stations[\"Station\"].values:\n",
    "            \n",
    "        data = pred_f[~pred_f[s].isnull()][[\"Id\",s]]\n",
    "        if len(data)==0:\n",
    "            continue\n",
    "        thrsd = thresholds[thresholds.Station==s].Threshold.values[0]\n",
    "        pred_class = pd.DataFrame({\"Id\" : data.Id\n",
    "                    ,\"pred_Response\" : [1 if x>thrsd else 0 for x in data[s].values]})\n",
    "        results.pred_Response[results.Id.isin(data.Id.values)] = \\\n",
    "                        pred_class[\"pred_Response\"].values\n",
    "            \n",
    "        print(\"Preditions done for station: \", s )\n",
    "        print(\"Predicted \", len(data), \"parts.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\pandas\\core\\generic.py:4485: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditions done for station:  pred_L2_S28\n",
      "Predicted  9583 parts.\n",
      "Preditions done for station:  pred_L3_S43\n",
      "Predicted  30551 parts.\n",
      "Preditions done for station:  pred_L3_S44\n",
      "Predicted  29804 parts.\n",
      "Preditions done for station:  pred_L3_S38\n",
      "Predicted  27142 parts.\n",
      "Preditions done for station:  pred_L3_S41\n",
      "Predicted  59913 parts.\n",
      "Preditions done for station:  pred_L3_S40\n",
      "Predicted  59914 parts.\n",
      "Preditions done for station:  pred_L3_S47\n",
      "Predicted  59955 parts.\n",
      "Preditions done for station:  pred_L1_S25\n",
      "Predicted  83658 parts.\n",
      "Preditions done for station:  pred_L3_S48\n",
      "Predicted  59923 parts.\n",
      "Preditions done for station:  pred_L3_S49\n",
      "Predicted  29673 parts.\n",
      "Preditions done for station:  pred_L3_S50\n",
      "Predicted  30359 parts.\n",
      "Preditions done for station:  pred_L0_S21\n",
      "Predicted  81409 parts.\n",
      "Preditions done for station:  pred_L0_S23\n",
      "Predicted  80290 parts.\n",
      "Preditions done for station:  pred_L0_S22\n",
      "Predicted  80599 parts.\n",
      "Preditions done for station:  pred_L0_S15\n",
      "Predicted  121445 parts.\n",
      "Preditions done for station:  pred_L2_S27\n",
      "Predicted  120729 parts.\n",
      "Preditions done for station:  pred_L0_S14\n",
      "Predicted  120625 parts.\n",
      "Preditions done for station:  pred_L0_S18\n",
      "Predicted  121081 parts.\n",
      "Preditions done for station:  pred_L0_S9\n",
      "Predicted  225678 parts.\n",
      "Preditions done for station:  pred_L1_S24\n",
      "Predicted  183727 parts.\n",
      "Preditions done for station:  pred_L0_S11\n",
      "Predicted  225452 parts.\n",
      "Preditions done for station:  pred_L0_S12\n",
      "Predicted  242061 parts.\n",
      "Preditions done for station:  pred_L2_S26\n",
      "Predicted  227011 parts.\n",
      "Preditions done for station:  pred_L0_S10\n",
      "Predicted  224540 parts.\n",
      "Preditions done for station:  pred_L0_S19\n",
      "Predicted  121027 parts.\n",
      "Preditions done for station:  pred_L0_S2\n",
      "Predicted  339774 parts.\n",
      "Preditions done for station:  pred_L0_S13\n",
      "Predicted  242065 parts.\n",
      "Preditions done for station:  pred_L0_S16\n",
      "Predicted  119139 parts.\n",
      "Preditions done for station:  pred_L0_S3\n",
      "Predicted  334708 parts.\n",
      "Preditions done for station:  pred_L3_S45\n",
      "Predicted  59932 parts.\n",
      "Preditions done for station:  pred_L0_S0\n",
      "Predicted  673862 parts.\n",
      "Preditions done for station:  pred_L3_S32\n",
      "Predicted  24543 parts.\n",
      "Preditions done for station:  pred_L0_S6\n",
      "Predicted  338988 parts.\n",
      "Preditions done for station:  pred_L0_S17\n",
      "Predicted  123027 parts.\n",
      "Preditions done for station:  pred_L0_S5\n",
      "Predicted  339512 parts.\n",
      "Preditions done for station:  pred_L0_S7\n",
      "Predicted  335698 parts.\n",
      "Preditions done for station:  pred_L3_S35\n",
      "Predicted  552108 parts.\n",
      "Preditions done for station:  pred_L3_S36\n",
      "Predicted  569032 parts.\n",
      "Preditions done for station:  pred_L3_S31\n",
      "Predicted  39003 parts.\n",
      "Preditions done for station:  pred_L3_S33\n",
      "Predicted  1114695 parts.\n",
      "Preditions done for station:  pred_L0_S1\n",
      "Predicted  673904 parts.\n",
      "Preditions done for station:  pred_L3_S29\n",
      "Predicted  1119629 parts.\n",
      "Preditions done for station:  pred_L0_S4\n",
      "Predicted  335295 parts.\n",
      "Preditions done for station:  pred_L3_S30\n",
      "Predicted  1119811 parts.\n",
      "Preditions done for station:  pred_L0_S8\n",
      "Predicted  673881 parts.\n",
      "Preditions done for station:  pred_L3_S34\n",
      "Predicted  1115118 parts.\n",
      "Preditions done for station:  pred_L0_S20\n",
      "Predicted  242111 parts.\n",
      "Preditions done for station:  pred_L3_S39\n",
      "Predicted  59908 parts.\n",
      "Preditions done for station:  pred_L3_S37\n",
      "Predicted  1120394 parts.\n",
      "Preditions done for station:  pred_L3_S51\n",
      "Predicted  59853 parts.\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%time train_predictions = predictId(pre_functs, stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1124715\n",
       "1      59032\n",
       "Name: pred_Response, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions.pred_Response.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.052486185389187485"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "59032/1124715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_predictions.to_csv(\"files/train_predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\pandas\\tools\\merge.py:205: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 3 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\pandas\\tools\\merge.py:478: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  self.right = self.right.drop(right_drop, axis=1)\n"
     ]
    }
   ],
   "source": [
    "train_predictions = train_predictions.merge(train.reset_index()[[\"Id\",\"Response\"]]\n",
    "                                            , on=\"Id\"\n",
    "                                            , how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_predictions.columns=[\"Id\",\"pred_Response\",\"Response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6562\n",
       "True      317\n",
       "Name: Id, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions[train_predictions.Response==1].Id.isin(\\\n",
    "        train_predictions[train_predictions.pred_Response==1].Id).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0013299303371982756"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(train_predictions.Response,train_predictions.pred_Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictClass(grps, mvas, results, processed=[], save=False):\n",
    "    for i,g in enumerate(grps.groups):\n",
    "        s=time.time()\n",
    "        if g in processed:\n",
    "            print(g, \" already predicted. Continue.\")\n",
    "            continue\n",
    "        # Dataset magic\n",
    "        d = grps.get_group(g).dropna(how=\"all\", axis=0)\n",
    "        d.columns = d.columns.droplevel([\"Line\",\"Station\"])\n",
    "        d=d.reset_index()\n",
    "        if len(d)==0:\n",
    "            continue\n",
    "        # pipeline\n",
    "        print(\"classifier pipeline prediction for goup: \"\n",
    "              , g, \"(\",i+1,\"/\",len(grps.groups),\")\")\n",
    "        clf = mvas[g]\n",
    "    \n",
    "        pred_func = pd.DataFrame({\"Id\" : d.Id\n",
    "                            ,\"pred_\"+g :clf.predict(d.drop(traindrop,axis=1))})\n",
    "        results = results.merge(pred_func, on=\"Id\", how=\"left\")\n",
    "        print(\"done in {:2.2f}\".format((time.time()-s)/60.),\" minutes.\")\n",
    "        del d\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_class = pd.DataFrame({\"Id\": train.reset_index()[\"Id\"],\n",
    "                       \"Response\": train.reset_index()[\"Response\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier pipeline prediction for goup:  L1_S24 ( 1 / 50 )\n",
      "done in 0.56  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S14 ( 2 / 50 )\n",
      "done in 0.31  minutes.\n",
      "classifier pipeline prediction for goup:  L2_S26 ( 3 / 50 )\n",
      "done in 0.56  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S39 ( 4 / 50 )\n",
      "done in 0.16  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S33 ( 5 / 50 )\n",
      "done in 2.72  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S31 ( 6 / 50 )\n",
      "done in 0.09  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S4 ( 7 / 50 )\n",
      "done in 0.80  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S30 ( 8 / 50 )\n",
      "done in 2.84  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S38 ( 9 / 50 )\n",
      "done in 0.07  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S29 ( 10 / 50 )\n",
      "done in 2.84  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S9 ( 11 / 50 )\n",
      "done in 0.58  minutes.\n",
      "classifier pipeline prediction for goup:  L2_S27 ( 12 / 50 )\n",
      "done in 0.32  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S21 ( 13 / 50 )\n",
      "done in 0.21  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S43 ( 14 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S49 ( 15 / 50 )\n",
      "done in 0.07  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S17 ( 16 / 50 )\n",
      "done in 0.31  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S19 ( 17 / 50 )\n",
      "done in 0.31  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S47 ( 18 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S45 ( 19 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S23 ( 20 / 50 )\n",
      "done in 0.21  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S6 ( 21 / 50 )\n",
      "done in 0.86  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S40 ( 22 / 50 )\n",
      "done in 0.18  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S18 ( 23 / 50 )\n",
      "done in 0.33  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S34 ( 24 / 50 )\n",
      "done in 2.70  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S20 ( 25 / 50 )\n",
      "done in 0.57  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S8 ( 26 / 50 )\n",
      "done in 1.66  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S0 ( 27 / 50 )\n",
      "done in 1.79  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S10 ( 28 / 50 )\n",
      "done in 0.58  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S2 ( 29 / 50 )\n",
      "done in 0.88  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S11 ( 30 / 50 )\n",
      "done in 0.59  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S36 ( 31 / 50 )\n",
      "done in 1.43  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S37 ( 32 / 50 )\n",
      "done in 2.69  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S51 ( 33 / 50 )\n",
      "done in 0.17  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S22 ( 34 / 50 )\n",
      "done in 0.21  minutes.\n",
      "classifier pipeline prediction for goup:  L1_S25 ( 35 / 50 )\n",
      "done in 0.36  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S35 ( 36 / 50 )\n",
      "done in 1.36  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S16 ( 37 / 50 )\n",
      "done in 0.30  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S13 ( 38 / 50 )\n",
      "done in 0.58  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S5 ( 39 / 50 )\n",
      "done in 0.82  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S3 ( 40 / 50 )\n",
      "done in 0.86  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S44 ( 41 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S50 ( 42 / 50 )\n",
      "done in 0.08  minutes.\n",
      "classifier pipeline prediction for goup:  L2_S28 ( 43 / 50 )\n",
      "done in 0.05  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S7 ( 44 / 50 )\n",
      "done in 0.84  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S12 ( 45 / 50 )\n",
      "done in 0.61  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S32 ( 46 / 50 )\n",
      "done in 0.07  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S41 ( 47 / 50 )\n",
      "done in 0.18  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S48 ( 48 / 50 )\n",
      "done in 0.18  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S15 ( 49 / 50 )\n",
      "done in 0.33  minutes.\n",
      "classifier pipeline prediction for goup:  L0_S1 ( 50 / 50 )\n",
      "done in 1.63  minutes.\n"
     ]
    }
   ],
   "source": [
    " pred_train_class = predictClass(gb,trained,result_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0    1176288\n",
       "NaN         580\n",
       "dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_class[pred_train_class.Response==0].drop([\"Id\",\"Response\"],axis=1).apply(pd.Series.sum,axis=1).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_train_class.pred_Response[pred_train_class.pred_Response.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    6826\n",
       "2.0      51\n",
       "dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_class[pred_train_class.Response==1].drop([\"Id\",\"Response\"],axis=1).apply(lambda x: pd.Series.sum(x,numeric_only=False),axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train_class[\"pred_Response\"] = pred_train_class.drop([\"Id\",\"Response\"],axis=1).apply(pd.Series.sum,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxxo_000\\Anaconda3\\envs\\jupnote\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "pred_train_class.pred_Response[pred_train_class.pred_Response.isnull()]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train_class.loc[:,\"pred_Response\"] = pred_train_class.loc[:,\"pred_Response\"]\\\n",
    ".apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1183696\n",
       "1         51\n",
       "Name: pred_Response, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_class.pred_Response.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train_class.to_csv(\"files/simple_prediction_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.085855124119177187"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(pred_train_class.Response, pred_train_class.pred_Response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go ahead an predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"files/test_numeric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a little bit of index magic to be able to group by station\n",
    "ids=test.Id\n",
    "test = test.set_index([\"Id\"])\n",
    "test.columns = addLineStation(test)\n",
    "\n",
    "gbt = test.groupby(level=\"Station\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame({\"Id\": test.reset_index()[\"Id\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traindrop=[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"L3_S40\" in trained.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trained.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier pipeline prediction for goup:  L3_S33 ( 1 / 50 )\n",
      "done in 2.69  minutes.\n",
      "classifier pipeline prediction for goup:  L3_S40 ( 2 / 50 )\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'L3_S40'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e0395bd87168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictStations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrained\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-cbf78c93d960>\u001b[0m in \u001b[0;36mpredictStations\u001b[0;34m(grps, mvas, results, processed, save)\u001b[0m\n\u001b[1;32m     14\u001b[0m         print(\"classifier pipeline prediction for goup: \"\n\u001b[1;32m     15\u001b[0m               , g, \"(\",i+1,\"/\",len(grps.groups),\")\")\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmvas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         pred_func = pd.DataFrame({\"Id\" : d.Id\n",
      "\u001b[0;31mKeyError\u001b[0m: 'L3_S40'"
     ]
    }
   ],
   "source": [
    "res_t = predictStations(gbt,trained,test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupnote]",
   "language": "python",
   "name": "conda-env-jupnote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
