{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Trained'  models\n",
    "* The first reference will be a model without any usage of knowledge. The probability to end up in the class for a test customer is assumed to be *n(customers per class)/all(customers)*.\n",
    "* The second model is a multi-class boosted decision tree classifier\n",
    "* The third model uses a boosted decision tree to classify male or female and a regression model to predict the age.\n",
    "\n",
    "---\n",
    "\n",
    "Using logistic loss evaluation the prediction scores:\n",
    "* Naive entries per class model: \n",
    "   - loss: **2.42786222642**\n",
    "   - score on kaggle: **2.42762**\n",
    "* AdaBoost Classifier with: \n",
    "   - algorithm='SAMME.R'\n",
    "   - DT(max_features=4, min_samples_leaf=74.645) \n",
    "   - learning_rate=0.15 \n",
    "   - n_estimators=800 \n",
    "   - random_state=666\n",
    "   - _loss on training_: **2.39243** // **2.53330** (Kaggle)\n",
    "   - _separate optimised BDTs for nEvts==0 and nEvts>=1_: **2.42243** (Kaggle)\n",
    "* Gradient Boosting:\n",
    "   - loss: deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/mschlupp/pythonTools\")\n",
    "tmp = %pwd\n",
    "files_dir = tmp + \"/files/\" \n",
    "solution_dir = tmp+'/predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv(files_dir+'gender_age_train.csv')\n",
    "#test = pd.read_csv(files_dir+'gender_age_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_events.csv        phone_brand_device_model.csv\r\n",
      "app_labels.csv        phone_brand_device_model_engl.csv\r\n",
      "events.csv            sample_submission.csv\r\n",
      "events_day_hour.csv   traintest_fullevt.csv\r\n",
      "gender_age_test.csv   traintest_phone.csv\r\n",
      "gender_age_train.csv  traintest_phone_day_hour.csv\r\n",
      "label_categories.csv  traintest_phone_evts.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls files/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us first remind ourself, what we have in our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_set = pd.read_csv(files_dir+'traintest_fullevt.csv', nrows=0) # just read the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'device_id', 'gender', 'group', 'isTrain', 'phone_brand',\n",
      "       'device_model', 'hasEvents', 'nEvts', 'longitude_mean',\n",
      "       'longitude_variance', 'latitude_mean', 'latitude_variance',\n",
      "       'usageTime_mean', 'usageTime_variance', 'usageDay_mean',\n",
      "       'usageDay_variance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols=new_set.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = cols.drop('hasEvents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's read the data chunkwise and split in train and test sample\n",
    "# we only want the training sample for now.\n",
    "data = pd.read_csv(files_dir+'traintest_fullevt.csv', usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=data[data.isTrain==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference  model\n",
    "The first reference will be a model without any usage of knowledge.\n",
    "The probability to end up in the class for a test customer is assumed to be *n(customers per class)/all(customers)*.\n",
    "\n",
    "---\n",
    "\n",
    "Using logistic loss evaluation the prediction scores:\n",
    "* Naive entries per class model: \n",
    "   - loss: **2.42786222642**\n",
    "   - score on kaggle: **2.42762**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = train.groupby('group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups.device_id = groups.device_id/len(train.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "F23-      0.067654\n",
       "F24-26    0.056132\n",
       "F27-28    0.041771\n",
       "F29-32    0.062000\n",
       "F33-42    0.074499\n",
       "F43+      0.056186\n",
       "M22-      0.100315\n",
       "M23-26    0.128676\n",
       "M27-28    0.072945\n",
       "M29-31    0.097917\n",
       "M32-38    0.126948\n",
       "M39+      0.114957\n",
       "Name: device_id, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.device_id # show our naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the prediction matrix\n",
    "prediction = np.zeros((len(train.age),len(groups.device_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us use the log_loss \n",
    "# (sklearn's logistic loss / cross-entropy) \n",
    "# implementation to score our prediction\n",
    "\n",
    "# first transform group into numerical classes\n",
    "labelEnc = LabelEncoder()\n",
    "labelEnc.fit(train.group)\n",
    "true_group = labelEnc.transform(train.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg = df(columns=groups.index.values)\n",
    "probs_per_group = dg.append(groups.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign our probabilities to the prediction array\n",
    "for i in range(0,prediction.shape[0]):\n",
    "    prediction[i]=probs_per_group.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss of our prediction is: \n",
      "2.42786222642\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic loss of our prediction is: \")\n",
    "print(log_loss(true_group,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class boosted decision tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold, LabelKFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our multi-class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes = pd.DataFrame(np.zeros((len(train.device_id),len(train.group.unique()))))\n",
    "true_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F23-', 'F24-26', 'F27-28', 'F29-32', 'F33-42', 'F43+', 'M22-',\n",
       "       'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+', 'none'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_groups = LabelEncoder()\n",
    "le_groups.fit(data.group.unique())\n",
    "le_groups.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_classes.columns = le_groups.inverse_transform(list(true_classes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still allive...  0\n",
      "still allive...  10001\n",
      "still allive...  20002\n",
      "still allive...  30003\n",
      "still allive...  40004\n",
      "still allive...  50005\n",
      "still allive...  60006\n",
      "still allive...  70007\n"
     ]
    }
   ],
   "source": [
    "# There should be a smarter way. It's late, sorry --> sparse matrices\n",
    "for i,row,x in zip(range(0,len(train.group)),true_classes.iterrows(), train.group):\n",
    "    if i % 10001 == 0: \n",
    "        print('still allive... ', i)\n",
    "    row[1][x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unorderable types: str() > int()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-98a0a6828220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrue_classes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_classes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0m_check_numpy_unicode_bug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y contains new labels: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36mintersect1d\u001b[1;34m(ar1, ar2, assume_unique)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mar2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unorderable types: str() > int()"
     ]
    }
   ],
   "source": [
    "true_classes.columns = le_groups.transform(true_classes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>nEvts</th>\n",
       "      <th>longitude_mean</th>\n",
       "      <th>longitude_variance</th>\n",
       "      <th>latitude_mean</th>\n",
       "      <th>latitude_variance</th>\n",
       "      <th>usageTime_mean</th>\n",
       "      <th>usageTime_variance</th>\n",
       "      <th>usageDay_mean</th>\n",
       "      <th>usageDay_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>749</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phone_brand  device_model  nEvts  longitude_mean  longitude_variance  \\\n",
       "0           51           749      0              -1                  -1   \n",
       "\n",
       "   latitude_mean  latitude_variance  usageTime_mean  usageTime_variance  \\\n",
       "0             -1                 -1              -1                  -1   \n",
       "\n",
       "   usageDay_mean  usageDay_variance  \n",
       "0             -1                 -1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataset we are actually using for training\n",
    "in_data = train.drop(['age','gender','group','device_id','isTrain'], axis=1)\n",
    "\n",
    "# now transform string variables into numerical categories \n",
    "le_phone = LabelEncoder()\n",
    "le_device = LabelEncoder()\n",
    "le_phone.fit(data['phone_brand'].unique())\n",
    "le_device.fit(data['device_model'].unique())\n",
    "\n",
    "in_data['device_model'] = le_device.transform(in_data['device_model'])\n",
    "in_data['phone_brand'] = le_phone.transform(in_data['phone_brand'])\n",
    "\n",
    "# let's check our dataset\n",
    "in_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, ...,  6, 10,  7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_class = le_groups.transform(train.group)\n",
    "true_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c50f895bb2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# no hyper-parameter tuning, just from experience...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m bdt = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=0.01*len(true_class.index)),\n\u001b[0m\u001b[0;32m      3\u001b[0m                          \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SAMME.R\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                          n_estimators=800)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# no hyper-parameter tuning, just from experience...\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=0.01*len(true_classes.index)),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         learning_rate=0.05,\n",
    "                         n_estimators=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=4, max_leaf_nodes=None, min_samples_leaf=74.645,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.15, n_estimators=800, random_state=666)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "s=time.time()\n",
    "bdt.fit(in_data,true_class)\n",
    "print('training finished after: ', (time.time()-s)/60.0, ' minutes.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'M23-26'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-d6826270283b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprobas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'duration of predictions: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' minutes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    259\u001b[0m                 isinstance(self.base_estimator,\n\u001b[0;32m    260\u001b[0m                            (BaseDecisionTree, BaseForest))):\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    371\u001b[0m                                       force_all_finite)\n\u001b[0;32m    372\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'M23-26'"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "probas = bdt.predict_proba(in_data)\n",
    "print('duration of predictions: ', (time.time()-s)/60.0, ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3924290904229317"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(true_classes,probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really overwhelming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=4, max_leaf_nodes=None, min_samples_leaf=74.645,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'),\n",
       " 'base_estimator__class_weight': None,\n",
       " 'base_estimator__criterion': 'gini',\n",
       " 'base_estimator__max_depth': None,\n",
       " 'base_estimator__max_features': 4,\n",
       " 'base_estimator__max_leaf_nodes': None,\n",
       " 'base_estimator__min_samples_leaf': 74.645,\n",
       " 'base_estimator__min_samples_split': 2,\n",
       " 'base_estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'base_estimator__presort': False,\n",
       " 'base_estimator__random_state': None,\n",
       " 'base_estimator__splitter': 'best',\n",
       " 'learning_rate': 0.15,\n",
       " 'n_estimators': 800,\n",
       " 'random_state': 666}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/ad_hoc_BDT.pkl']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bdt, 'trainedModels/ad_hoc_BDT.pkl',compress=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdt = joblib.load('trainedModels/ad_hoc_BDT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py:676: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  proba = proba[:, :self.n_classes_]\n"
     ]
    }
   ],
   "source": [
    "# let's prepare a submission\n",
    "probs_naive_df = pd.DataFrame(bdt.predict_proba(test), index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-ce977bc414ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprobs_naive_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs_naive_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y contains new labels: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "probs_naive_df.columns = le_groups.inverse_transform(probs_naive_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = pd.concat([pd.DataFrame(data.device_id[data.isTrain==0]),probs_naive_df], axis=1)\n",
    "out.to_csv(solution_dir+'naive_bdt.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current data set up, two very different prediction scenarios are presend: either the data has event entries, or it doesn't. We'll test if two optimised algorithms for each scenario help to reduce the loss score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimisePars(mva, points, data , classes, fraction=0.7, score = 'log_loss', cvs=5):\n",
    "    '''\n",
    "    Funtion to optimise hyper-parameters. Follows sklearn example:\n",
    "    \"example-model-selection-grid-search-digits-py\"\n",
    "    \n",
    "    Arguments:\n",
    "    mva - multivariate method to optimise\n",
    "    points - dictionary that holds optimisation poitns\n",
    "    data - your training data\n",
    "    classes - true categories/classes/labels \n",
    "    fraction - fraction of training/test split\n",
    "    score - score function to optimies the classifier\n",
    "    cvs - number of cross-validation folds or cross-validation generator\n",
    "    \n",
    "    To-Do:\n",
    "    - classification report does not exactly work every time. \n",
    "    '''\n",
    "    import time\n",
    "    print(\"# Tuning hyper-parameters for log_loss score\")\n",
    "    \n",
    "    # Splits data\n",
    "    data_train, data_test, classes_train, classes_test =  train_test_split(\n",
    "    data, classes, test_size=fraction, random_state=0)\n",
    "    s =  time.time()\n",
    "    clf = GridSearchCV(mva, points, cv=cvs,\n",
    "                       scoring=score, n_jobs=4, \n",
    "                       verbose=2)\n",
    "                       \n",
    "    clf.fit(data_train, classes_train)\n",
    "\n",
    "    print('GridSearch completed after ', (time.time()-s)/60.0, ' minutes.')\n",
    "    print()\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = classes_test, clf.predict_proba(data_test)\n",
    "    print(\"Log loss score on test sample: \", log_loss(y_true, y_pred))\n",
    "    \n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimise BDT Parameters\n",
    "We use sklearn GridSearchCV method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_data['group'] = train.group\n",
    "data_hasEvts = in_data.query('nEvts>0')\n",
    "class_hasEvts =  le_groups.transform(data_hasEvts.group)\n",
    "data_noEvts = in_data.query('nEvts==0').drop(['nEvts','longitude_mean', 'longitude_variance',\n",
    "       'latitude_mean', 'latitude_variance', 'usageTime_mean',\n",
    "       'usageTime_variance', 'usageDay_mean', 'usageDay_variance'],axis=1)\n",
    "class_noEvts =  le_groups.transform(data_noEvts.group)\n",
    "\n",
    "data_hasEvts = data_hasEvts.drop(['group'],axis=1);\n",
    "data_noEvts = data_noEvts.drop(['group'],axis=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tune new models\n",
    "bdt_noEvts = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\")\n",
    "\n",
    "\n",
    "bdt_hasEvts = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of points considered in the optimisation\n",
    "bdt_pars = {'learning_rate': [0.005, 0.001, 0.0005],\n",
    "             'n_estimators': [300,800,500],\n",
    " #'base_estimator__min_samples_leaf': [400,200,300]\n",
    "            'base_estimator__max_depth': [2,3,4]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "\n",
      "GridSearch completed after  25.57805508375168  minutes.\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "-2.427 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.451 (+/-0.001) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.439 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.413 (+/-0.004) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.418 (+/-0.003) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.415 (+/-0.003) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.412 (+/-0.004) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.414 (+/-0.004) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.412 (+/-0.004) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.423 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.448 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.436 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.409 (+/-0.006) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.414 (+/-0.005) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.410 (+/-0.005) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.409 (+/-0.008) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.410 (+/-0.006) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.409 (+/-0.007) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.425 (+/-0.008) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.445 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.434 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.012) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.419 (+/-0.008) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.008) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.419 (+/-0.015) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.010) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.013) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full test set.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [ 7464 30802]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-026e1471da0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m '''\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m \u001b[0mgrid_bdt_noEvts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimisePars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbdt_noEvts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbdt_pars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_noEvts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_noEvts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-192-8e696f30ca8b>\u001b[0m in \u001b[0;36moptimisePars\u001b[1;34m(mva, points, data, classes, fraction, score, cvs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     \u001b[1;31m# Check if dimensions are consistent.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1560\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1561\u001b[0m     \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[1;32m--> 176\u001b[1;33m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [ 7464 30802]"
     ]
    }
   ],
   "source": [
    "# first try:\n",
    "# -2.472 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "# second try:\n",
    "# -2.437 (+/-0.003) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "# third\n",
    "# -2.411 (+/-0.013) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "# fourth\n",
    "# -2.408 (+/-0.011) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.001, 'n_estimators': 300}\n",
    "# fifth\n",
    "# -2.408 (+/-0.009) for {'base_estimator__min_samples_leaf': 400, 'learning_rate': 0.001, 'n_estimators': 300}\n",
    "# sixth\n",
    "# -2.409 (+/-0.007) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
    "\n",
    "'''\n",
    "Grid scores on training set:\n",
    "~52 min (36 points)\n",
    "\n",
    "-4.124 (+/-0.069) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-3.208 (+/-0.091) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-3.615 (+/-0.116) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-4.600 (+/-0.138) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-4.220 (+/-0.079) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-4.466 (+/-0.099) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-3.059 (+/-0.086) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.621 (+/-0.037) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.774 (+/-0.054) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "-2.610 (+/-0.073) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-2.613 (+/-0.073) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-2.614 (+/-0.070) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-2.607 (+/-0.104) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.609 (+/-0.076) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.607 (+/-0.091) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.616 (+/-0.074) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.594 (+/-0.067) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.614 (+/-0.080) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "-2.482 (+/-0.021) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-2.484 (+/-0.017) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-2.483 (+/-0.018) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-2.476 (+/-0.027) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.482 (+/-0.022) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.480 (+/-0.024) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.485 (+/-0.022) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.487 (+/-0.016) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.487 (+/-0.022) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-2.483 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-2.482 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-2.472 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.477 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.483 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.484 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.484 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "\n",
    "~20 min (18 points)\n",
    "-2.471 (+/-0.001) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.479 (+/-0.000) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.476 (+/-0.000) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.437 (+/-0.003) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "-2.462 (+/-0.001) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 800}\n",
    "-2.451 (+/-0.002) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 500}\n",
    "-2.472 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.477 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.439 (+/-0.003) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "-2.463 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.01, 'n_estimators': 800}\n",
    "-2.453 (+/-0.002) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.01, 'n_estimators': 500}\n",
    "-2.473 (+/-0.001) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.478 (+/-0.000) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.440 (+/-0.003) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "-2.464 (+/-0.001) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.01, 'n_estimators': 800}\n",
    "-2.454 (+/-0.001) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.01, 'n_estimators': 500}\n",
    "\n",
    "~3 min (18 points)\n",
    "-2.610 (+/-0.118) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "-2.611 (+/-0.129) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 100}\n",
    "-2.612 (+/-0.134) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 200}\n",
    "-2.616 (+/-0.105) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.001, 'n_estimators': 50}\n",
    "-2.613 (+/-0.106) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.001, 'n_estimators': 100}\n",
    "-2.610 (+/-0.123) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.001, 'n_estimators': 200}\n",
    "-2.442 (+/-0.055) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "-2.441 (+/-0.049) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.005, 'n_estimators': 100}\n",
    "-2.439 (+/-0.046) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.005, 'n_estimators': 200}\n",
    "-2.450 (+/-0.054) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.001, 'n_estimators': 50}\n",
    "-2.447 (+/-0.053) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.001, 'n_estimators': 100}\n",
    "-2.443 (+/-0.055) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.001, 'n_estimators': 200}\n",
    "-2.411 (+/-0.013) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "-2.410 (+/-0.011) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 100}\n",
    "-2.413 (+/-0.009) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 200}\n",
    "-2.415 (+/-0.015) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.001, 'n_estimators': 50}\n",
    "-2.413 (+/-0.014) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.001, 'n_estimators': 100}\n",
    "-2.412 (+/-0.013) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.001, 'n_estimators': 200}\n",
    "\n",
    "~ 11 min (18 points)\n",
    "~ 21 min (27 points)\n",
    "~ 26 min (27 points)\n",
    "'''\n",
    "\n",
    "grid_bdt_noEvts = optimisePars(bdt_noEvts, bdt_pars, data_noEvts, class_noEvts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the AdaBoost DT with\n",
    "**Loss: -2.408 (+/-0.009) for {'base_estimator__min_samples_leaf': 400, 'learning_rate': 0.001, 'n_estimators': 300}**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimisation resulted in error, so we set it by hand \n",
    "bdt_noEvts = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=400),\n",
    "                                learning_rate=0.001,\n",
    "                                n_estimators=300,\n",
    "                                algorithm=\"SAMME.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split in train and test sets\n",
    "noE_train, noE_test, noE_class_train, noE_class_test = train_test_split(data_noEvts, class_noEvts, \n",
    "                                                                        test_size = 0.2, random_state=0,\n",
    "                                                                        stratify=class_noEvts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=400,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.001, n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the bdt\n",
    "bdt_noEvts.fit(noE_train, noE_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict test set\n",
    "probas = bdt_noEvts.predict_proba(noE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss score:  2.40527688085\n"
     ]
    }
   ],
   "source": [
    "# print log_loss of this model\n",
    "print('log loss score: ', log_loss(noE_class_test, probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, performs as expected: loss of ~2.408 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/optimised_bdtnoEvts.pkl']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle result to be able to skip training the next time\n",
    "joblib.dump(bdt_noEvts, 'trainedModels/optimised_bdtnoEvts.pkl',compress=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load bdt\n",
    "bdt_noEvts = joblib.load('trainedModels/optimised_bdtnoEvts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "\n",
      "GridSearch completed after  56.55024749437968  minutes.\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "-2.401 (+/-0.003) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.433 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.417 (+/-0.003) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.383 (+/-0.006) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.389 (+/-0.004) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.385 (+/-0.005) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.383 (+/-0.007) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.384 (+/-0.006) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.383 (+/-0.006) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.424 (+/-0.003) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.453 (+/-0.002) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.440 (+/-0.002) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.397 (+/-0.007) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.424 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.411 (+/-0.005) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.385 (+/-0.015) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.386 (+/-0.013) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.383 (+/-0.016) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.386 (+/-0.009) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.385 (+/-0.017) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.384 (+/-0.009) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.417 (+/-0.005) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.439 (+/-0.003) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.429 (+/-0.004) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.398 (+/-0.018) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.412 (+/-0.010) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.405 (+/-0.014) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.415 (+/-0.052) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.396 (+/-0.032) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.403 (+/-0.037) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.434 (+/-0.057) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.408 (+/-0.044) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.420 (+/-0.057) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.407 (+/-0.012) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.421 (+/-0.009) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.414 (+/-0.009) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.419 (+/-0.019) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.404 (+/-0.009) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.407 (+/-0.010) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.508 (+/-0.052) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.444 (+/-0.030) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.474 (+/-0.046) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.571 (+/-0.096) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.492 (+/-0.050) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.522 (+/-0.061) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.403 (+/-0.012) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.403 (+/-0.011) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.401 (+/-0.011) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.489 (+/-0.033) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.436 (+/-0.015) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.460 (+/-0.031) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "-2.735 (+/-0.108) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.562 (+/-0.050) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.646 (+/-0.077) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "-2.810 (+/-0.096) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.682 (+/-0.071) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.745 (+/-0.078) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "-2.444 (+/-0.027) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.410 (+/-0.015) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.421 (+/-0.018) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "Log loss score on test sample:  2.38986295322\n"
     ]
    }
   ],
   "source": [
    "# optimise bdt for the data with events\n",
    "# list of points considered in the optimisation\n",
    "bdt_pars = {'learning_rate': [0.005, 0.001, 0.0005, 0.01],\n",
    "             'n_estimators': [300,800,500],\n",
    " #'base_estimator__min_samples_leaf': [400,200,300]\n",
    "            'base_estimator__max_depth': [2,3,4,5,6]\n",
    "           }\n",
    "\n",
    "grid_bdt_hasEvts = optimisePars(bdt_hasEvts, bdt_pars, data_hasEvts, class_hasEvts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle bdt result\n",
    "joblib.dump(grid_bdt_hasEvts.best_estimator_, 'trainedModels/optimised_bdthasEvts.pkl',compress=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load result (you can skip the training steps)\n",
    "bdt_hasEvts = joblib.load('trainedModels/optimised_bdthasEvts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how the two BDTs work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = data[data.isTrain==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.drop(['age','device_id','gender','group', 'isTrain'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['phone_brand'] = le_phone.transform(test.phone_brand)\n",
    "test['device_model'] = le_device.transform(test.device_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hasEvts_i = test[test.nEvts>=1].index\n",
    "noEvts_i = test[test.nEvts==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_evts = test.loc[hasEvts_i]\n",
    "test_noevts = test.loc[noEvts_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas_evts = bdt_hasEvts.predict_proba(test_evts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_noevts = bdt_noEvts.predict_proba(test_noevts.drop(['nEvts','longitude_mean', 'longitude_variance',\n",
    "       'latitude_mean', 'latitude_variance', 'usageTime_mean',\n",
    "       'usageTime_variance', 'usageDay_mean', 'usageDay_variance'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data.device_id[data.isTrain==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_evts_df = pd.DataFrame(probas_evts, index=hasEvts_i, columns=le_groups.inverse_transform(range(0,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_noevts_df = pd.DataFrame(probas_noevts, index=noEvts_i, columns=le_groups.inverse_transform(range(0,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = pd.concat([submission.loc[hasEvts_i],probas_evts_df], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = pd.concat([submission.loc[noEvts_i],probas_noevts_df], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm = pd.concat([n,nn], join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm.sort_index();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm.to_csv(solution_dir+'separate_bdt.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Try GradientBoosting\n",
    "first build the simple gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(loss='deviance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes_gbtrain = le_groups.transform(train.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 out of  24 | elapsed: 69.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch completed after  77.24419992764791  minutes.\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'min_samples_leaf': 800, 'n_estimators': 1200, 'learning_rate': 0.001}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "-2.416 (+/-0.003) for {'min_samples_leaf': 800, 'n_estimators': 900, 'learning_rate': 0.001}\n",
      "-2.411 (+/-0.003) for {'min_samples_leaf': 800, 'n_estimators': 1200, 'learning_rate': 0.001}\n",
      "-2.418 (+/-0.002) for {'min_samples_leaf': 1300, 'n_estimators': 900, 'learning_rate': 0.001}\n",
      "-2.412 (+/-0.003) for {'min_samples_leaf': 1300, 'n_estimators': 1200, 'learning_rate': 0.001}\n",
      "-2.464 (+/-0.000) for {'min_samples_leaf': 800, 'n_estimators': 900, 'learning_rate': 0.0001}\n",
      "-2.460 (+/-0.000) for {'min_samples_leaf': 800, 'n_estimators': 1200, 'learning_rate': 0.0001}\n",
      "-2.464 (+/-0.000) for {'min_samples_leaf': 1300, 'n_estimators': 900, 'learning_rate': 0.0001}\n",
      "-2.461 (+/-0.000) for {'min_samples_leaf': 1300, 'n_estimators': 1200, 'learning_rate': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "Log loss score on test sample:  2.40840699034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Tuning hyper-parameters for log_loss score\\nFitting 3 folds for each of 36 candidates, totalling 108 fits\\n\\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 53.2min\\n[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed: 137.8min finished\\n\\nGridSearch completed after  146.37849520047504  minutes.\\n\\nBest parameters set found on training set:\\n\\n{'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\\n\\nGrid scores on training set:\\n\\n-2.651 (+/-0.033) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.850 (+/-0.044) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.515 (+/-0.022) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.619 (+/-0.025) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.462 (+/-0.017) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.518 (+/-0.023) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.607 (+/-0.027) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.798 (+/-0.038) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.496 (+/-0.018) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.582 (+/-0.017) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.447 (+/-0.016) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.491 (+/-0.023) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.405 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.408 (+/-0.004) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.404 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.409 (+/-0.002) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.455 (+/-0.018) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.525 (+/-0.024) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.429 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.459 (+/-0.016) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.416 (+/-0.010) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.433 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.440 (+/-0.014) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.496 (+/-0.021) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.423 (+/-0.013) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.449 (+/-0.015) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.412 (+/-0.009) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.426 (+/-0.012) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\\n\\nDetailed classification report:\\n\\nThe model is trained on the full training set.\\nThe scores are computed on the full test set.\\n\\nLog loss score on test sample:  2.39880543668\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary holding omtimisation points\n",
    "GBpars = {'n_estimators': [900,1200],\n",
    "          'min_samples_leaf' : [800,1300],\n",
    "          'learning_rate' : [0.001, 0.0001]\n",
    "          #,'max_features' : [None, 'sqrt']\n",
    "         }\n",
    "\n",
    "grid_gbdt = optimisePars(gbdt, GBpars, in_data.drop([\"group\"], axis=1), classes_gbtrain, cvs=3)\n",
    "\n",
    "'''\n",
    "# Tuning hyper-parameters for log_loss score\n",
    "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
    "\n",
    "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 53.2min\n",
    "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed: 137.8min finished\n",
    "\n",
    "GridSearch completed after  146.37849520047504  minutes.\n",
    "\n",
    "Best parameters set found on training set:\n",
    "\n",
    "{'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "\n",
    "Grid scores on training set:\n",
    "\n",
    "-2.651 (+/-0.033) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.850 (+/-0.044) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.515 (+/-0.022) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.619 (+/-0.025) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.462 (+/-0.017) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.518 (+/-0.023) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.607 (+/-0.027) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.798 (+/-0.038) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.496 (+/-0.018) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.582 (+/-0.017) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.447 (+/-0.016) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.491 (+/-0.023) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.405 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.408 (+/-0.004) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.404 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.409 (+/-0.002) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.455 (+/-0.018) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.525 (+/-0.024) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.429 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.459 (+/-0.016) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.416 (+/-0.010) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.433 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.440 (+/-0.014) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.496 (+/-0.021) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.423 (+/-0.013) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.449 (+/-0.015) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.412 (+/-0.009) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.426 (+/-0.012) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "\n",
    "Detailed classification report:\n",
    "\n",
    "The model is trained on the full training set.\n",
    "The scores are computed on the full test set.\n",
    "\n",
    "Log loss score on test sample:  2.39880543668\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(loss='deviance', max_features=None\n",
    "                                  , min_samples_leaf=800\n",
    "                                  , learning_rate=0.005\n",
    "                                  , n_estimators=700);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.005, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=800, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=700,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt.fit(in_data.drop('group', axis=1), classes_gbtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/gbdt_fullset.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(gbdt, 'trainedModels/gbdt_fullset.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbdt = joblib.load(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the submission\n",
    "First create a matrix for the predictions of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-3b550a4eb05e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# assign our probabilities to the prediction array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs_per_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((len(test.device_id),len(groups.index.values)))\n",
    "# assign our probabilities to the prediction array\n",
    "for i in range(0,prediction.shape[0]):\n",
    "    prediction[i]=probs_per_group.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define function that prepares the valid submission csv\n",
    "It uses the test dataset and the prediction matrix as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareOutput(test, pred, label='talkingData'):\n",
    "    '''\n",
    "    Writes an valid submission file from the prediction matrix.\n",
    "    The valid output must look like: \n",
    "    device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\n",
    "    (id, probailities)\n",
    "\n",
    "    Arguments:\n",
    "    test  - the DataFrame with the device_id's to be tested\n",
    "    pred  - is the prediction matrix with pred.shape = (len(test.device_id,len(unique groups))\n",
    "    label - prefix of the submission file\n",
    "    \n",
    "    Return:\n",
    "    The merged submission dataset is returned.\n",
    "    '''\n",
    "    p = pd.DataFrame(pred)\n",
    "    p.columns = labelEnc.inverse_transform(p.columns)\n",
    "    i = pd.DataFrame(test.device_id.values) \n",
    "    i.columns = ['device_id']\n",
    "    merged= pd.concat([i,p], axis=1)\n",
    "    merged.to_csv(solution_dir+label+'_submission.csv', index=False)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entriesPerClass_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "o = prepareOutput(test,prediction,'entriesPerClass')\n",
    "%ls predictions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>F23-</th>\n",
       "      <th>F24-26</th>\n",
       "      <th>F27-28</th>\n",
       "      <th>F29-32</th>\n",
       "      <th>F33-42</th>\n",
       "      <th>F43+</th>\n",
       "      <th>M22-</th>\n",
       "      <th>M23-26</th>\n",
       "      <th>M27-28</th>\n",
       "      <th>M29-31</th>\n",
       "      <th>M32-38</th>\n",
       "      <th>M39+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002079943728939269</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074499</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.114957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1547860181818787117</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074499</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.114957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             device_id      F23-    F24-26    F27-28  F29-32    F33-42  \\\n",
       "0  1002079943728939269  0.067654  0.056132  0.041771   0.062  0.074499   \n",
       "1 -1547860181818787117  0.067654  0.056132  0.041771   0.062  0.074499   \n",
       "\n",
       "       F43+      M22-    M23-26    M27-28    M29-31    M32-38      M39+  \n",
       "0  0.056186  0.100315  0.128676  0.072945  0.097917  0.126948  0.114957  \n",
       "1  0.056186  0.100315  0.128676  0.072945  0.097917  0.126948  0.114957  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This worked. The ouput can be submitted to kaggle."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
