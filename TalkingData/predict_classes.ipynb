{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Trained'  models\n",
    "* The first reference will be a model without any usage of knowledge. The probability to end up in the class for a test customer is assumed to be *n(customers per class)/all(customers)*.\n",
    "* The second model is a multi-class boosted decision tree classifier\n",
    "* The third model uses a boosted decision tree to classify male or female and a regression model to predict the age.\n",
    "\n",
    "---\n",
    "\n",
    "Using logistic loss evaluation the prediction scores:\n",
    "* Naive entries per class model: \n",
    "   - loss: **2.42786222642**\n",
    "   - score on kaggle: **2.42762**\n",
    "* AdaBoost Classifier with: \n",
    "   - algorithm='SAMME.R'\n",
    "   - DT(max_features=4, min_samples_leaf=74.645) \n",
    "   - learning_rate=0.15 , n_estimators=800 \n",
    "   - random_state=666\n",
    "   - _loss on training_: **2.39243** // **2.53330** (Kaggle)\n",
    "   - _separate optimised BDTs for nEvts==0 and nEvts>=1_: **2.42243** (Kaggle)\n",
    "* Gradient Boosting:\n",
    "   - loss: deviance\n",
    "   - max_features=None, min_samples_leaf=800, \n",
    "   - learning_rate=0.005, n_estimators=700\n",
    "   - score on Kaggle: 2.39166 (local part of training data: 2.3988)\n",
    "* SVM Solution:\n",
    "   - C=1.1, gamma='auto', kernel='rbf'\n",
    "   - score on Kaggle: 2.40673\n",
    "   - but 80 minutes training\n",
    "* Neural net (Lasagne&Theano):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/mschlupp/pythonTools\")\n",
    "tmp = %pwd\n",
    "files_dir = tmp + \"/files/\" \n",
    "solution_dir = tmp+'/predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv(files_dir+'gender_age_train.csv')\n",
    "#test = pd.read_csv(files_dir+'gender_age_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_events.csv        phone_brand_device_model.csv\r\n",
      "app_labels.csv        phone_brand_device_model_engl.csv\r\n",
      "events.csv            sample_submission.csv\r\n",
      "events_day_hour.csv   traintest_fullevt.csv\r\n",
      "gender_age_test.csv   traintest_phone.csv\r\n",
      "gender_age_train.csv  traintest_phone_day_hour.csv\r\n",
      "label_categories.csv  traintest_phone_evts.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls files/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us first remind ourself, what we have in our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_set = pd.read_csv(files_dir+'traintest_fullevt.csv', nrows=0) # just read the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'device_id', 'gender', 'group', 'isTrain', 'phone_brand',\n",
      "       'device_model', 'hasEvents', 'nEvts', 'longitude_mean',\n",
      "       'longitude_variance', 'latitude_mean', 'latitude_variance',\n",
      "       'usageTime_mean', 'usageTime_variance', 'usageDay_mean',\n",
      "       'usageDay_variance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols=new_set.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = cols.drop('hasEvents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's read the data chunkwise and split in train and test sample\n",
    "# we only want the training sample for now.\n",
    "data = pd.read_csv(files_dir+'traintest_fullevt.csv', usecols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=data[data.isTrain==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our multi-class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes = pd.DataFrame(np.zeros((len(train.device_id),len(train.group.unique()))))\n",
    "true_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F23-', 'F24-26', 'F27-28', 'F29-32', 'F33-42', 'F43+', 'M22-',\n",
       "       'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+', 'none'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_groups = LabelEncoder()\n",
    "le_groups.fit(data.group.unique())\n",
    "le_groups.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_classes.columns = le_groups.inverse_transform(list(true_classes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F23-', 'F24-26', 'F27-28', 'F29-32', 'F33-42', 'F43+', 'M22-',\n",
       "       'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still allive...  0\n",
      "still allive...  10001\n",
      "still allive...  20002\n",
      "still allive...  30003\n",
      "still allive...  40004\n",
      "still allive...  50005\n",
      "still allive...  60006\n",
      "still allive...  70007\n"
     ]
    }
   ],
   "source": [
    "# There should be a smarter way. It's late, sorry --> sparse matrices\n",
    "for i,row,x in zip(range(0,len(train.group)),true_classes.iterrows(), train.group):\n",
    "    if i % 10001 == 0: \n",
    "        print('still allive... ', i)\n",
    "    row[1][x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_classes.columns = le_groups.transform(true_classes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>nEvts</th>\n",
       "      <th>longitude_mean</th>\n",
       "      <th>longitude_variance</th>\n",
       "      <th>latitude_mean</th>\n",
       "      <th>latitude_variance</th>\n",
       "      <th>usageTime_mean</th>\n",
       "      <th>usageTime_variance</th>\n",
       "      <th>usageDay_mean</th>\n",
       "      <th>usageDay_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>749</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phone_brand  device_model  nEvts  longitude_mean  longitude_variance  \\\n",
       "0           51           749      0              -1                  -1   \n",
       "\n",
       "   latitude_mean  latitude_variance  usageTime_mean  usageTime_variance  \\\n",
       "0             -1                 -1              -1                  -1   \n",
       "\n",
       "   usageDay_mean  usageDay_variance  \n",
       "0             -1                 -1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataset we are actually using for training\n",
    "in_data = train.drop(['age','gender','group','device_id','isTrain'], axis=1)\n",
    "\n",
    "# now transform string variables into numerical categories \n",
    "le_phone = LabelEncoder()\n",
    "le_device = LabelEncoder()\n",
    "le_phone.fit(data['phone_brand'].unique())\n",
    "le_device.fit(data['device_model'].unique())\n",
    "\n",
    "in_data['device_model'] = le_device.transform(in_data['device_model'])\n",
    "in_data['phone_brand'] = le_phone.transform(in_data['phone_brand'])\n",
    "\n",
    "# let's check our dataset\n",
    "in_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, ...,  6, 10,  7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_class = le_groups.transform(train.group)\n",
    "true_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference  model\n",
    "The first reference will be a model without any usage of knowledge.\n",
    "The probability to end up in the class for a test customer is assumed to be *n(customers per class)/all(customers)*.\n",
    "\n",
    "---\n",
    "\n",
    "Using logistic loss evaluation the prediction scores:\n",
    "* Naive entries per class model: \n",
    "   - loss: **2.42786222642**\n",
    "   - score on kaggle: **2.42762**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = train.groupby('group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups.device_id = groups.device_id/len(train.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "F23-      0.067654\n",
       "F24-26    0.056132\n",
       "F27-28    0.041771\n",
       "F29-32    0.062000\n",
       "F33-42    0.074499\n",
       "F43+      0.056186\n",
       "M22-      0.100315\n",
       "M23-26    0.128676\n",
       "M27-28    0.072945\n",
       "M29-31    0.097917\n",
       "M32-38    0.126948\n",
       "M39+      0.114957\n",
       "Name: device_id, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.device_id # show our naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the prediction matrix\n",
    "prediction = np.zeros((len(train.age),len(groups.device_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us use the log_loss \n",
    "# (sklearn's logistic loss / cross-entropy) \n",
    "# implementation to score our prediction\n",
    "\n",
    "# first transform group into numerical classes\n",
    "labelEnc = LabelEncoder()\n",
    "labelEnc.fit(train.group)\n",
    "true_group = labelEnc.transform(train.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg = df(columns=groups.index.values)\n",
    "probs_per_group = dg.append(groups.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign our probabilities to the prediction array\n",
    "for i in range(0,prediction.shape[0]):\n",
    "    prediction[i]=probs_per_group.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss of our prediction is: \n",
      "2.42786222642\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic loss of our prediction is: \")\n",
    "print(log_loss(true_group,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class boosted decision tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold, LabelKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c50f895bb2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# no hyper-parameter tuning, just from experience...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m bdt = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=0.01*len(true_class.index)),\n\u001b[0m\u001b[0;32m      3\u001b[0m                          \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SAMME.R\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                          n_estimators=800)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# no hyper-parameter tuning, just from experience...\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=0.01*len(true_classes.index)),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         learning_rate=0.05,\n",
    "                         n_estimators=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=4, max_leaf_nodes=None, min_samples_leaf=74.645,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.15, n_estimators=800, random_state=666)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=time.time()\n",
    "bdt.fit(in_data,true_class)\n",
    "print('training finished after: ', (time.time()-s)/60.0, ' minutes.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'M23-26'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-d6826270283b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprobas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'duration of predictions: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' minutes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    259\u001b[0m                 isinstance(self.base_estimator,\n\u001b[0;32m    260\u001b[0m                            (BaseDecisionTree, BaseForest))):\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    371\u001b[0m                                       force_all_finite)\n\u001b[0;32m    372\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'M23-26'"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "probas = bdt.predict_proba(in_data)\n",
    "print('duration of predictions: ', (time.time()-s)/60.0, ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3924290904229317"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(true_classes,probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really overwhelming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=4, max_leaf_nodes=None, min_samples_leaf=74.645,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'),\n",
       " 'base_estimator__class_weight': None,\n",
       " 'base_estimator__criterion': 'gini',\n",
       " 'base_estimator__max_depth': None,\n",
       " 'base_estimator__max_features': 4,\n",
       " 'base_estimator__max_leaf_nodes': None,\n",
       " 'base_estimator__min_samples_leaf': 74.645,\n",
       " 'base_estimator__min_samples_split': 2,\n",
       " 'base_estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'base_estimator__presort': False,\n",
       " 'base_estimator__random_state': None,\n",
       " 'base_estimator__splitter': 'best',\n",
       " 'learning_rate': 0.15,\n",
       " 'n_estimators': 800,\n",
       " 'random_state': 666}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/ad_hoc_BDT.pkl']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bdt, 'trainedModels/ad_hoc_BDT.pkl',compress=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bdt = joblib.load('trainedModels/ad_hoc_BDT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/tree/tree.py:676: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  proba = proba[:, :self.n_classes_]\n"
     ]
    }
   ],
   "source": [
    "# let's prepare a submission\n",
    "probs_naive_df = pd.DataFrame(bdt.predict_proba(test), index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-ce977bc414ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprobs_naive_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs_naive_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y contains new labels: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "probs_naive_df.columns = le_groups.inverse_transform(probs_naive_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = pd.concat([pd.DataFrame(data.device_id[data.isTrain==0]),probs_naive_df], axis=1)\n",
    "out.to_csv(solution_dir+'naive_bdt.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current data set up, two very different prediction scenarios are presend: either the data has event entries, or it doesn't. We'll test if two optimised algorithms for each scenario help to reduce the loss score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimisePars(mva, points, data , classes, fraction=0.7, score = 'log_loss', cvs=5):\n",
    "    '''\n",
    "    Funtion to optimise hyper-parameters. Follows sklearn example:\n",
    "    \"example-model-selection-grid-search-digits-py\"\n",
    "    \n",
    "    Arguments:\n",
    "    mva - multivariate method to optimise\n",
    "    points - dictionary that holds optimisation poitns\n",
    "    data - your training data\n",
    "    classes - true categories/classes/labels \n",
    "    fraction - fraction of training/test split\n",
    "    score - score function to optimies the classifier\n",
    "    cvs - number of cross-validation folds or cross-validation generator\n",
    "    \n",
    "    To-Do:\n",
    "    - classification report does not exactly work every time. \n",
    "    '''\n",
    "    import time\n",
    "    print(\"# Tuning hyper-parameters for log_loss score\")\n",
    "    \n",
    "    # Splits data\n",
    "    data_train, data_test, classes_train, classes_test =  train_test_split(\n",
    "    data, classes, test_size=fraction, random_state=0)\n",
    "    s =  time.time()\n",
    "    clf = GridSearchCV(mva, points, cv=cvs,\n",
    "                       scoring=score, n_jobs=4, \n",
    "                       verbose=2)\n",
    "                       \n",
    "    clf.fit(data_train, classes_train)\n",
    "\n",
    "    print('GridSearch completed after ', (time.time()-s)/60.0, ' minutes.')\n",
    "    print()\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = classes_test, clf.predict_proba(data_test)\n",
    "    print(\"Log loss score on test sample: \", log_loss(y_true, y_pred))\n",
    "    \n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimise BDT Parameters\n",
    "We use sklearn GridSearchCV method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f67ab4adae6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0min_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'group'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_hasEvts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nEvts>0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclass_hasEvts\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mle_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_hasEvts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m data_noEvts = in_data.query('nEvts==0').drop(['nEvts','longitude_mean', 'longitude_variance',\n\u001b[0;32m      5\u001b[0m        \u001b[1;34m'latitude_mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'latitude_variance'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'usageTime_mean'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "in_data['group'] = train.group\n",
    "data_hasEvts = in_data.query('nEvts>0')\n",
    "class_hasEvts =  le_groups.transform(data_hasEvts.group)\n",
    "data_noEvts = in_data.query('nEvts==0').drop(['nEvts','longitude_mean', 'longitude_variance',\n",
    "       'latitude_mean', 'latitude_variance', 'usageTime_mean',\n",
    "       'usageTime_variance', 'usageDay_mean', 'usageDay_variance'],axis=1)\n",
    "class_noEvts =  le_groups.transform(data_noEvts.group)\n",
    "\n",
    "data_hasEvts = data_hasEvts.drop(['group'],axis=1);\n",
    "data_noEvts = data_noEvts.drop(['group'],axis=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tune new models\n",
    "bdt_noEvts = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\")\n",
    "\n",
    "\n",
    "bdt_hasEvts = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of points considered in the optimisation\n",
    "bdt_pars = {'learning_rate': [0.005, 0.001, 0.0005],\n",
    "             'n_estimators': [300,800,500],\n",
    " #'base_estimator__min_samples_leaf': [400,200,300]\n",
    "            'base_estimator__max_depth': [2,3,4]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "\n",
      "GridSearch completed after  25.57805508375168  minutes.\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "-2.427 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.451 (+/-0.001) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.439 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.413 (+/-0.004) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.418 (+/-0.003) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.415 (+/-0.003) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.412 (+/-0.004) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.414 (+/-0.004) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.412 (+/-0.004) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.423 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.448 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.436 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.409 (+/-0.006) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.414 (+/-0.005) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.410 (+/-0.005) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.409 (+/-0.008) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.410 (+/-0.006) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.409 (+/-0.007) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.425 (+/-0.008) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.445 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.434 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.012) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.419 (+/-0.008) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.008) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.419 (+/-0.015) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.010) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.417 (+/-0.013) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full test set.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [ 7464 30802]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-026e1471da0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m '''\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m \u001b[0mgrid_bdt_noEvts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimisePars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbdt_noEvts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbdt_pars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_noEvts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_noEvts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-192-8e696f30ca8b>\u001b[0m in \u001b[0;36moptimisePars\u001b[1;34m(mva, points, data, classes, fraction, score, cvs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight)\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m     \u001b[1;31m# Check if dimensions are consistent.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1560\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1561\u001b[0m     \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[1;32m--> 176\u001b[1;33m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [ 7464 30802]"
     ]
    }
   ],
   "source": [
    "# first try:\n",
    "# -2.472 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "# second try:\n",
    "# -2.437 (+/-0.003) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "# third\n",
    "# -2.411 (+/-0.013) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "# fourth\n",
    "# -2.408 (+/-0.011) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.001, 'n_estimators': 300}\n",
    "# fifth\n",
    "# -2.408 (+/-0.009) for {'base_estimator__min_samples_leaf': 400, 'learning_rate': 0.001, 'n_estimators': 300}\n",
    "# sixth\n",
    "# -2.409 (+/-0.007) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
    "\n",
    "'''\n",
    "Grid scores on training set:\n",
    "~52 min (36 points)\n",
    "\n",
    "-4.124 (+/-0.069) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-3.208 (+/-0.091) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-3.615 (+/-0.116) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-4.600 (+/-0.138) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-4.220 (+/-0.079) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-4.466 (+/-0.099) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-3.059 (+/-0.086) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.621 (+/-0.037) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.774 (+/-0.054) for {'base_estimator__min_samples_leaf': 10, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "-2.610 (+/-0.073) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-2.613 (+/-0.073) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-2.614 (+/-0.070) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-2.607 (+/-0.104) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.609 (+/-0.076) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.607 (+/-0.091) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.616 (+/-0.074) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.594 (+/-0.067) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.614 (+/-0.080) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "-2.482 (+/-0.021) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-2.484 (+/-0.017) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-2.483 (+/-0.018) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-2.476 (+/-0.027) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.482 (+/-0.022) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.480 (+/-0.024) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.485 (+/-0.022) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.487 (+/-0.016) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.487 (+/-0.022) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.15, 'n_estimators': 300}\n",
    "-2.483 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.15, 'n_estimators': 800}\n",
    "-2.482 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.15, 'n_estimators': 500}\n",
    "-2.472 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.477 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.483 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.5, 'n_estimators': 300}\n",
    "-2.484 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.5, 'n_estimators': 800}\n",
    "-2.484 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.5, 'n_estimators': 500}\n",
    "\n",
    "~20 min (18 points)\n",
    "-2.471 (+/-0.001) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.479 (+/-0.000) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.476 (+/-0.000) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.437 (+/-0.003) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "-2.462 (+/-0.001) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 800}\n",
    "-2.451 (+/-0.002) for {'base_estimator__min_samples_leaf': 300, 'learning_rate': 0.01, 'n_estimators': 500}\n",
    "-2.472 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.477 (+/-0.000) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.439 (+/-0.003) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "-2.463 (+/-0.001) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.01, 'n_estimators': 800}\n",
    "-2.453 (+/-0.002) for {'base_estimator__min_samples_leaf': 500, 'learning_rate': 0.01, 'n_estimators': 500}\n",
    "-2.473 (+/-0.001) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.05, 'n_estimators': 300}\n",
    "-2.480 (+/-0.000) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.05, 'n_estimators': 800}\n",
    "-2.478 (+/-0.000) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "-2.440 (+/-0.003) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.01, 'n_estimators': 300}\n",
    "-2.464 (+/-0.001) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.01, 'n_estimators': 800}\n",
    "-2.454 (+/-0.001) for {'base_estimator__min_samples_leaf': 1000, 'learning_rate': 0.01, 'n_estimators': 500}\n",
    "\n",
    "~3 min (18 points)\n",
    "-2.610 (+/-0.118) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "-2.611 (+/-0.129) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 100}\n",
    "-2.612 (+/-0.134) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 200}\n",
    "-2.616 (+/-0.105) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.001, 'n_estimators': 50}\n",
    "-2.613 (+/-0.106) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.001, 'n_estimators': 100}\n",
    "-2.610 (+/-0.123) for {'base_estimator__min_samples_leaf': 50, 'learning_rate': 0.001, 'n_estimators': 200}\n",
    "-2.442 (+/-0.055) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "-2.441 (+/-0.049) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.005, 'n_estimators': 100}\n",
    "-2.439 (+/-0.046) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.005, 'n_estimators': 200}\n",
    "-2.450 (+/-0.054) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.001, 'n_estimators': 50}\n",
    "-2.447 (+/-0.053) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.001, 'n_estimators': 100}\n",
    "-2.443 (+/-0.055) for {'base_estimator__min_samples_leaf': 100, 'learning_rate': 0.001, 'n_estimators': 200}\n",
    "-2.411 (+/-0.013) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 50}\n",
    "-2.410 (+/-0.011) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 100}\n",
    "-2.413 (+/-0.009) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.005, 'n_estimators': 200}\n",
    "-2.415 (+/-0.015) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.001, 'n_estimators': 50}\n",
    "-2.413 (+/-0.014) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.001, 'n_estimators': 100}\n",
    "-2.412 (+/-0.013) for {'base_estimator__min_samples_leaf': 200, 'learning_rate': 0.001, 'n_estimators': 200}\n",
    "\n",
    "~ 11 min (18 points)\n",
    "~ 21 min (27 points)\n",
    "~ 26 min (27 points)\n",
    "'''\n",
    "\n",
    "grid_bdt_noEvts = optimisePars(bdt_noEvts, bdt_pars, data_noEvts, class_noEvts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the AdaBoost DT with\n",
    "**Loss: -2.408 (+/-0.009) for {'base_estimator__min_samples_leaf': 400, 'learning_rate': 0.001, 'n_estimators': 300}**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimisation resulted in error, so we set it by hand \n",
    "bdt_noEvts = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=400),\n",
    "                                learning_rate=0.001,\n",
    "                                n_estimators=300,\n",
    "                                algorithm=\"SAMME.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split in train and test sets\n",
    "noE_train, noE_test, noE_class_train, noE_class_test = train_test_split(data_noEvts, class_noEvts, \n",
    "                                                                        test_size = 0.2, random_state=0,\n",
    "                                                                        stratify=class_noEvts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=400,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.001, n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the bdt\n",
    "bdt_noEvts.fit(noE_train, noE_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict test set\n",
    "probas = bdt_noEvts.predict_proba(noE_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss score:  2.40527688085\n"
     ]
    }
   ],
   "source": [
    "# print log_loss of this model\n",
    "print('log loss score: ', log_loss(noE_class_test, probas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, performs as expected: loss of ~2.408 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/optimised_bdtnoEvts.pkl']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle result to be able to skip training the next time\n",
    "joblib.dump(bdt_noEvts, 'trainedModels/optimised_bdtnoEvts.pkl',compress=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load bdt\n",
    "bdt_noEvts = joblib.load('trainedModels/optimised_bdtnoEvts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "\n",
      "GridSearch completed after  56.55024749437968  minutes.\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "-2.401 (+/-0.003) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.433 (+/-0.002) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.417 (+/-0.003) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.383 (+/-0.006) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.389 (+/-0.004) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.385 (+/-0.005) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.383 (+/-0.007) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.384 (+/-0.006) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.383 (+/-0.006) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.424 (+/-0.003) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 2}\n",
      "-2.453 (+/-0.002) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 2}\n",
      "-2.440 (+/-0.002) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 2}\n",
      "-2.397 (+/-0.007) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.424 (+/-0.004) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.411 (+/-0.005) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.385 (+/-0.015) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.386 (+/-0.013) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.383 (+/-0.016) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.386 (+/-0.009) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.385 (+/-0.017) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.384 (+/-0.009) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.417 (+/-0.005) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 3}\n",
      "-2.439 (+/-0.003) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 3}\n",
      "-2.429 (+/-0.004) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 3}\n",
      "-2.398 (+/-0.018) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.412 (+/-0.010) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.405 (+/-0.014) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.415 (+/-0.052) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.396 (+/-0.032) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.403 (+/-0.037) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.434 (+/-0.057) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.408 (+/-0.044) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.420 (+/-0.057) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.407 (+/-0.012) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 4}\n",
      "-2.421 (+/-0.009) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 4}\n",
      "-2.414 (+/-0.009) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 4}\n",
      "-2.419 (+/-0.019) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.404 (+/-0.009) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.407 (+/-0.010) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.508 (+/-0.052) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.444 (+/-0.030) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.474 (+/-0.046) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.571 (+/-0.096) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.492 (+/-0.050) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.522 (+/-0.061) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.403 (+/-0.012) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 5}\n",
      "-2.403 (+/-0.011) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 5}\n",
      "-2.401 (+/-0.011) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 5}\n",
      "-2.489 (+/-0.033) for {'learning_rate': 0.005, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.436 (+/-0.015) for {'learning_rate': 0.005, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.460 (+/-0.031) for {'learning_rate': 0.005, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "-2.735 (+/-0.108) for {'learning_rate': 0.001, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.562 (+/-0.050) for {'learning_rate': 0.001, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.646 (+/-0.077) for {'learning_rate': 0.001, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "-2.810 (+/-0.096) for {'learning_rate': 0.0005, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.682 (+/-0.071) for {'learning_rate': 0.0005, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.745 (+/-0.078) for {'learning_rate': 0.0005, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "-2.444 (+/-0.027) for {'learning_rate': 0.01, 'n_estimators': 300, 'base_estimator__max_depth': 6}\n",
      "-2.410 (+/-0.015) for {'learning_rate': 0.01, 'n_estimators': 800, 'base_estimator__max_depth': 6}\n",
      "-2.421 (+/-0.018) for {'learning_rate': 0.01, 'n_estimators': 500, 'base_estimator__max_depth': 6}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "Log loss score on test sample:  2.38986295322\n"
     ]
    }
   ],
   "source": [
    "# optimise bdt for the data with events\n",
    "# list of points considered in the optimisation\n",
    "bdt_pars = {'learning_rate': [0.005, 0.001, 0.0005, 0.01],\n",
    "             'n_estimators': [300,800,500],\n",
    " #'base_estimator__min_samples_leaf': [400,200,300]\n",
    "            'base_estimator__max_depth': [2,3,4,5,6]\n",
    "           }\n",
    "\n",
    "grid_bdt_hasEvts = optimisePars(bdt_hasEvts, bdt_pars, data_hasEvts, class_hasEvts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle bdt result\n",
    "joblib.dump(grid_bdt_hasEvts.best_estimator_, 'trainedModels/optimised_bdthasEvts.pkl',compress=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load result (you can skip the training steps)\n",
    "bdt_hasEvts = joblib.load('trainedModels/optimised_bdthasEvts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how the two BDTs work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = data[data.isTrain==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.drop(['age','device_id','gender','group', 'isTrain'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['phone_brand'] = le_phone.transform(test.phone_brand)\n",
    "test['device_model'] = le_device.transform(test.device_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hasEvts_i = test[test.nEvts>=1].index\n",
    "noEvts_i = test[test.nEvts==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_evts = test.loc[hasEvts_i]\n",
    "test_noevts = test.loc[noEvts_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas_evts = bdt_hasEvts.predict_proba(test_evts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_noevts = bdt_noEvts.predict_proba(test_noevts.drop(['nEvts','longitude_mean', 'longitude_variance',\n",
    "       'latitude_mean', 'latitude_variance', 'usageTime_mean',\n",
    "       'usageTime_variance', 'usageDay_mean', 'usageDay_variance'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data.device_id[data.isTrain==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_evts_df = pd.DataFrame(probas_evts, index=hasEvts_i, columns=le_groups.inverse_transform(range(0,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_noevts_df = pd.DataFrame(probas_noevts, index=noEvts_i, columns=le_groups.inverse_transform(range(0,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = pd.concat([submission.loc[hasEvts_i],probas_evts_df], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = pd.concat([submission.loc[noEvts_i],probas_noevts_df], join='inner', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm = pd.concat([n,nn], join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm.sort_index();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm.to_csv(solution_dir+'separate_bdt.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# Try GradientBoosting\n",
    "first build the simple gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(loss='deviance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes_gbtrain = le_groups.transform(train.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 out of  24 | elapsed: 69.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch completed after  77.24419992764791  minutes.\n",
      "\n",
      "Best parameters set found on training set:\n",
      "\n",
      "{'min_samples_leaf': 800, 'n_estimators': 1200, 'learning_rate': 0.001}\n",
      "\n",
      "Grid scores on training set:\n",
      "\n",
      "-2.416 (+/-0.003) for {'min_samples_leaf': 800, 'n_estimators': 900, 'learning_rate': 0.001}\n",
      "-2.411 (+/-0.003) for {'min_samples_leaf': 800, 'n_estimators': 1200, 'learning_rate': 0.001}\n",
      "-2.418 (+/-0.002) for {'min_samples_leaf': 1300, 'n_estimators': 900, 'learning_rate': 0.001}\n",
      "-2.412 (+/-0.003) for {'min_samples_leaf': 1300, 'n_estimators': 1200, 'learning_rate': 0.001}\n",
      "-2.464 (+/-0.000) for {'min_samples_leaf': 800, 'n_estimators': 900, 'learning_rate': 0.0001}\n",
      "-2.460 (+/-0.000) for {'min_samples_leaf': 800, 'n_estimators': 1200, 'learning_rate': 0.0001}\n",
      "-2.464 (+/-0.000) for {'min_samples_leaf': 1300, 'n_estimators': 900, 'learning_rate': 0.0001}\n",
      "-2.461 (+/-0.000) for {'min_samples_leaf': 1300, 'n_estimators': 1200, 'learning_rate': 0.0001}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full training set.\n",
      "The scores are computed on the full test set.\n",
      "\n",
      "Log loss score on test sample:  2.40840699034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Tuning hyper-parameters for log_loss score\\nFitting 3 folds for each of 36 candidates, totalling 108 fits\\n\\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 53.2min\\n[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed: 137.8min finished\\n\\nGridSearch completed after  146.37849520047504  minutes.\\n\\nBest parameters set found on training set:\\n\\n{'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\\n\\nGrid scores on training set:\\n\\n-2.651 (+/-0.033) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.850 (+/-0.044) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.515 (+/-0.022) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.619 (+/-0.025) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.462 (+/-0.017) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.518 (+/-0.023) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.607 (+/-0.027) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.798 (+/-0.038) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.496 (+/-0.018) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.582 (+/-0.017) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.447 (+/-0.016) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\\n-2.491 (+/-0.023) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\\n-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.405 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.408 (+/-0.004) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.404 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.409 (+/-0.002) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\\n-2.403 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\\n-2.455 (+/-0.018) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.525 (+/-0.024) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.429 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.459 (+/-0.016) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.416 (+/-0.010) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.433 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.440 (+/-0.014) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.496 (+/-0.021) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.423 (+/-0.013) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.449 (+/-0.015) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\\n-2.412 (+/-0.009) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\\n-2.426 (+/-0.012) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\\n\\nDetailed classification report:\\n\\nThe model is trained on the full training set.\\nThe scores are computed on the full test set.\\n\\nLog loss score on test sample:  2.39880543668\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary holding omtimisation points\n",
    "GBpars = {'n_estimators': [900,1200],\n",
    "          'min_samples_leaf' : [800,1300],\n",
    "          'learning_rate' : [0.001, 0.0001]\n",
    "          #,'max_features' : [None, 'sqrt']\n",
    "         }\n",
    "\n",
    "#grid_gbdt = optimisePars(gbdt, GBpars, in_data.drop([\"group\"], axis=1), classes_gbtrain, cvs=3)\n",
    "\n",
    "'''\n",
    "# Tuning hyper-parameters for log_loss score\n",
    "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
    "\n",
    "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 53.2min\n",
    "[Parallel(n_jobs=4)]: Done 108 out of 108 | elapsed: 137.8min finished\n",
    "\n",
    "GridSearch completed after  146.37849520047504  minutes.\n",
    "\n",
    "Best parameters set found on training set:\n",
    "\n",
    "{'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "\n",
    "Grid scores on training set:\n",
    "\n",
    "-2.651 (+/-0.033) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.850 (+/-0.044) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.515 (+/-0.022) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.619 (+/-0.025) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.462 (+/-0.017) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.518 (+/-0.023) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.607 (+/-0.027) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.798 (+/-0.038) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.496 (+/-0.018) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.582 (+/-0.017) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.447 (+/-0.016) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 300}\n",
    "-2.491 (+/-0.023) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.4, 'n_estimators': 700}\n",
    "-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.405 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.408 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.007) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.408 (+/-0.004) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.005) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.404 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.409 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.006) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.409 (+/-0.002) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 300}\n",
    "-2.403 (+/-0.004) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.005, 'n_estimators': 700}\n",
    "-2.455 (+/-0.018) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.525 (+/-0.024) for {'max_features': None, 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.429 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.459 (+/-0.016) for {'max_features': None, 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.416 (+/-0.010) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.433 (+/-0.012) for {'max_features': None, 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.440 (+/-0.014) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.496 (+/-0.021) for {'max_features': 'sqrt', 'min_samples_leaf': 50, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.423 (+/-0.013) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.449 (+/-0.015) for {'max_features': 'sqrt', 'min_samples_leaf': 300, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "-2.412 (+/-0.009) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 300}\n",
    "-2.426 (+/-0.012) for {'max_features': 'sqrt', 'min_samples_leaf': 800, 'learning_rate': 0.1, 'n_estimators': 700}\n",
    "\n",
    "Detailed classification report:\n",
    "\n",
    "The model is trained on the full training set.\n",
    "The scores are computed on the full test set.\n",
    "\n",
    "Log loss score on test sample:  2.39880543668\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbdt = GradientBoostingClassifier(loss='deviance', max_features=None\n",
    "                                  , min_samples_leaf=800\n",
    "                                  , learning_rate=0.005\n",
    "                                  , n_estimators=700);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.005, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=800, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=700,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt.fit(in_data.drop('group', axis=1), classes_gbtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/gbdt_fullset.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(gbdt, 'trainedModels/gbdt_fullset.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbdt = joblib.load(\"trainedModels/gbdt_fullset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample = data[data.isTrain==0].drop(['age','gender','device_id','group','isTrain'],axis=1)\n",
    "test_sample[\"phone_brand\"] = le_phone.transform(test_sample.phone_brand)\n",
    "test_sample[\"device_model\"] = le_device.transform(test_sample.device_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probas_gbdt = gbdt.predict_proba(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare gbdt submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_gbdt_df = pd.DataFrame(probas_gbdt, index=test_sample.index)\n",
    "probas_gbdt_df.columns = le_groups.inverse_transform(probas_gbdt_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = pd.concat([pd.DataFrame(data[data.isTrain==0].device_id),probas_gbdt_df], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.to_csv('predictions/gbdt_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=1.1, cache_size=500\n",
    "    , decision_function_shape='ovo'\n",
    "    , gamma='auto', kernel='rbf'\n",
    "    , verbose=1, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]SVM trained in  79.19006366729737  minutes\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "svm.fit(in_data.drop('group', axis=1), classes_gbtrain)\n",
    "print(\"SVM trained in \", (time.time()-s)/60.0, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas_svm_df =  pd.DataFrame(svm.predict_proba(test_sample)\n",
    "                              , index=test_sample.index\n",
    "                              , columns=le_groups.inverse_transform(range(0,12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = pd.concat([pd.DataFrame(data[data.isTrain==0].device_id),probas_svm_df], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.to_csv('predictions/svm_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a neural net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>nEvts</th>\n",
       "      <th>longitude_mean</th>\n",
       "      <th>longitude_variance</th>\n",
       "      <th>latitude_mean</th>\n",
       "      <th>latitude_variance</th>\n",
       "      <th>usageTime_mean</th>\n",
       "      <th>usageTime_variance</th>\n",
       "      <th>usageDay_mean</th>\n",
       "      <th>usageDay_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>749</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>749</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phone_brand  device_model  nEvts  longitude_mean  longitude_variance  \\\n",
       "0           51           749      0              -1                  -1   \n",
       "1           51           749      0              -1                  -1   \n",
       "\n",
       "   latitude_mean  latitude_variance  usageTime_mean  usageTime_variance  \\\n",
       "0             -1                 -1              -1                  -1   \n",
       "1             -1                 -1              -1                  -1   \n",
       "\n",
       "   usageDay_mean  usageDay_variance  \n",
       "0             -1                 -1  \n",
       "1             -1                 -1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_classes_nn = le_groups.transform(train.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74645, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_data.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_classes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "from theano.tensor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-4a13d22a8922>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msharedvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "dim = sha(in_data.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Input 0 is incompatible with layer dense_6: expected ndim=2, found ndim=3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-a3f94d0e334c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# first we add a dense layer (std NN layer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# we need an output of 12 dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m74645\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# no real motivation for relu here\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                     \u001b[0minput_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_input_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mcreate_input_layer\u001b[1;34m(self, batch_input_shape, input_dtype, name)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[1;31m# raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;31m# with the input_spec specified in the layer constructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;31m# collect input shapes to build layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mschlupp/anaconda3/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    406\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m                                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                         str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    409\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Input 0 is incompatible with layer dense_6: expected ndim=2, found ndim=3"
     ]
    }
   ],
   "source": [
    "# in keras we need to build models.\n",
    "# we build our own sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# first we add a dense layer (std NN layer)\n",
    "# we need an output of 12 dimensions\n",
    "model.add(Dense(output_dim=20, input_shape=(11,74645)))\n",
    "model.add(Activation(\"relu\")) # no real motivation for relu here\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_dim=25))\n",
    "model.add(Activation(\"relu\")) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(output_dim=15))\n",
    "model.add(Activation(\"relu\")) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(output_dim=15))\n",
    "model.add(Activation(\"relu\")) \n",
    "model.add(Dense(output_dim=12))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we need to configure the learning process\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we would like to use the categorical_crossentropy (multiclass logloss)\n",
    "# so let's convert classes to categroies\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_classes_nn = to_categorical(true_classes_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59716 samples, validate on 14929 samples\n",
      "Epoch 1/100\n",
      "1s - loss: 2.4338 - acc: 0.1264 - val_loss: 2.4007 - val_acc: 0.1385\n",
      "Epoch 2/100\n",
      "1s - loss: 2.4335 - acc: 0.1241 - val_loss: 2.4040 - val_acc: 0.1523\n",
      "Epoch 3/100\n",
      "1s - loss: 2.4335 - acc: 0.1267 - val_loss: 2.3959 - val_acc: 0.1320\n",
      "Epoch 4/100\n",
      "1s - loss: 2.4333 - acc: 0.1268 - val_loss: 2.4035 - val_acc: 0.1565\n",
      "Epoch 5/100\n",
      "1s - loss: 2.4342 - acc: 0.1256 - val_loss: 2.4055 - val_acc: 0.1291\n",
      "Epoch 6/100\n",
      "1s - loss: 2.4333 - acc: 0.1270 - val_loss: 2.4078 - val_acc: 0.0919\n",
      "Epoch 7/100\n",
      "1s - loss: 2.4332 - acc: 0.1265 - val_loss: 2.4151 - val_acc: 0.0918\n",
      "Epoch 8/100\n",
      "1s - loss: 2.4335 - acc: 0.1262 - val_loss: 2.4029 - val_acc: 0.1504\n",
      "Epoch 9/100\n",
      "1s - loss: 2.4334 - acc: 0.1265 - val_loss: 2.3990 - val_acc: 0.1576\n",
      "Epoch 10/100\n",
      "1s - loss: 2.4328 - acc: 0.1265 - val_loss: 2.4014 - val_acc: 0.1538\n",
      "Epoch 11/100\n",
      "1s - loss: 2.4322 - acc: 0.1267 - val_loss: 2.4025 - val_acc: 0.1474\n",
      "Epoch 12/100\n",
      "1s - loss: 2.4312 - acc: 0.1273 - val_loss: 2.3994 - val_acc: 0.1395\n",
      "Epoch 13/100\n",
      "1s - loss: 2.4315 - acc: 0.1266 - val_loss: 2.4047 - val_acc: 0.1525\n",
      "Epoch 14/100\n",
      "1s - loss: 2.4337 - acc: 0.1271 - val_loss: 2.4063 - val_acc: 0.1575\n",
      "Epoch 15/100\n",
      "1s - loss: 2.4318 - acc: 0.1295 - val_loss: 2.3927 - val_acc: 0.1397\n",
      "Epoch 16/100\n",
      "1s - loss: 2.4310 - acc: 0.1302 - val_loss: 2.4016 - val_acc: 0.1336\n",
      "Epoch 17/100\n",
      "1s - loss: 2.4319 - acc: 0.1302 - val_loss: 2.4063 - val_acc: 0.1354\n",
      "Epoch 18/100\n",
      "1s - loss: 2.4310 - acc: 0.1276 - val_loss: 2.3994 - val_acc: 0.1388\n",
      "Epoch 19/100\n",
      "1s - loss: 2.4307 - acc: 0.1287 - val_loss: 2.4097 - val_acc: 0.1509\n",
      "Epoch 20/100\n",
      "1s - loss: 2.4300 - acc: 0.1307 - val_loss: 2.3934 - val_acc: 0.1418\n",
      "Epoch 21/100\n",
      "1s - loss: 2.4303 - acc: 0.1316 - val_loss: 2.3971 - val_acc: 0.1605\n",
      "Epoch 22/100\n",
      "1s - loss: 2.4300 - acc: 0.1304 - val_loss: 2.3938 - val_acc: 0.1563\n",
      "Epoch 23/100\n",
      "1s - loss: 2.4297 - acc: 0.1306 - val_loss: 2.4012 - val_acc: 0.1377\n",
      "Epoch 24/100\n",
      "1s - loss: 2.4321 - acc: 0.1289 - val_loss: 2.3962 - val_acc: 0.1395\n",
      "Epoch 25/100\n",
      "1s - loss: 2.4323 - acc: 0.1290 - val_loss: 2.4054 - val_acc: 0.0922\n",
      "Epoch 26/100\n",
      "1s - loss: 2.4330 - acc: 0.1273 - val_loss: 2.3994 - val_acc: 0.1391\n",
      "Epoch 27/100\n",
      "1s - loss: 2.4331 - acc: 0.1288 - val_loss: 2.4032 - val_acc: 0.1548\n",
      "Epoch 28/100\n",
      "1s - loss: 2.4323 - acc: 0.1289 - val_loss: 2.3966 - val_acc: 0.1556\n",
      "Epoch 29/100\n",
      "1s - loss: 2.4310 - acc: 0.1289 - val_loss: 2.4056 - val_acc: 0.0977\n",
      "Epoch 30/100\n",
      "1s - loss: 2.4312 - acc: 0.1284 - val_loss: 2.4070 - val_acc: 0.0942\n",
      "Epoch 31/100\n",
      "1s - loss: 2.4300 - acc: 0.1310 - val_loss: 2.3975 - val_acc: 0.1443\n",
      "Epoch 32/100\n",
      "1s - loss: 2.4288 - acc: 0.1333 - val_loss: 2.3948 - val_acc: 0.1452\n",
      "Epoch 33/100\n",
      "1s - loss: 2.4311 - acc: 0.1346 - val_loss: 2.3998 - val_acc: 0.1452\n",
      "Epoch 34/100\n",
      "1s - loss: 2.4294 - acc: 0.1340 - val_loss: 2.4038 - val_acc: 0.1184\n",
      "Epoch 35/100\n",
      "1s - loss: 2.4289 - acc: 0.1338 - val_loss: 2.4066 - val_acc: 0.1155\n",
      "Epoch 36/100\n",
      "1s - loss: 2.4281 - acc: 0.1351 - val_loss: 2.4121 - val_acc: 0.1331\n",
      "Epoch 37/100\n",
      "1s - loss: 2.4296 - acc: 0.1328 - val_loss: 2.4014 - val_acc: 0.1541\n",
      "Epoch 38/100\n",
      "1s - loss: 2.4285 - acc: 0.1331 - val_loss: 2.3988 - val_acc: 0.1029\n",
      "Epoch 39/100\n",
      "1s - loss: 2.4304 - acc: 0.1310 - val_loss: 2.4064 - val_acc: 0.1482\n",
      "Epoch 40/100\n",
      "1s - loss: 2.4291 - acc: 0.1314 - val_loss: 2.3939 - val_acc: 0.1451\n",
      "Epoch 41/100\n",
      "1s - loss: 2.4285 - acc: 0.1314 - val_loss: 2.3907 - val_acc: 0.1614\n",
      "Epoch 42/100\n",
      "1s - loss: 2.4293 - acc: 0.1299 - val_loss: 2.4018 - val_acc: 0.1407\n",
      "Epoch 43/100\n",
      "1s - loss: 2.4284 - acc: 0.1324 - val_loss: 2.4006 - val_acc: 0.1347\n",
      "Epoch 44/100\n",
      "1s - loss: 2.4272 - acc: 0.1330 - val_loss: 2.3962 - val_acc: 0.1444\n",
      "Epoch 45/100\n",
      "1s - loss: 2.4274 - acc: 0.1321 - val_loss: 2.4065 - val_acc: 0.1417\n",
      "Epoch 46/100\n",
      "1s - loss: 2.4276 - acc: 0.1334 - val_loss: 2.3945 - val_acc: 0.1444\n",
      "Epoch 47/100\n",
      "1s - loss: 2.4298 - acc: 0.1307 - val_loss: 2.3962 - val_acc: 0.1510\n",
      "Epoch 48/100\n",
      "1s - loss: 2.4292 - acc: 0.1303 - val_loss: 2.3932 - val_acc: 0.1455\n",
      "Epoch 49/100\n",
      "1s - loss: 2.4272 - acc: 0.1332 - val_loss: 2.3989 - val_acc: 0.1508\n",
      "Epoch 50/100\n",
      "1s - loss: 2.4273 - acc: 0.1336 - val_loss: 2.4047 - val_acc: 0.1406\n",
      "Epoch 51/100\n",
      "1s - loss: 2.4275 - acc: 0.1315 - val_loss: 2.3890 - val_acc: 0.1521\n",
      "Epoch 52/100\n",
      "1s - loss: 2.4245 - acc: 0.1359 - val_loss: 2.4018 - val_acc: 0.1425\n",
      "Epoch 53/100\n",
      "1s - loss: 2.4218 - acc: 0.1389 - val_loss: 2.3937 - val_acc: 0.1452\n",
      "Epoch 54/100\n",
      "1s - loss: 2.4235 - acc: 0.1374 - val_loss: 2.3934 - val_acc: 0.1510\n",
      "Epoch 55/100\n",
      "1s - loss: 2.4233 - acc: 0.1321 - val_loss: 2.3962 - val_acc: 0.1414\n",
      "Epoch 56/100\n",
      "1s - loss: 2.4233 - acc: 0.1338 - val_loss: 2.3910 - val_acc: 0.1531\n",
      "Epoch 57/100\n",
      "1s - loss: 2.4250 - acc: 0.1299 - val_loss: 2.3929 - val_acc: 0.1492\n",
      "Epoch 58/100\n",
      "1s - loss: 2.4243 - acc: 0.1327 - val_loss: 2.3958 - val_acc: 0.1412\n",
      "Epoch 59/100\n",
      "1s - loss: 2.4262 - acc: 0.1301 - val_loss: 2.3999 - val_acc: 0.1534\n",
      "Epoch 60/100\n",
      "1s - loss: 2.4258 - acc: 0.1317 - val_loss: 2.4009 - val_acc: 0.0985\n",
      "Epoch 61/100\n",
      "1s - loss: 2.4239 - acc: 0.1324 - val_loss: 2.4024 - val_acc: 0.1264\n",
      "Epoch 62/100\n",
      "1s - loss: 2.4239 - acc: 0.1325 - val_loss: 2.3844 - val_acc: 0.1578\n",
      "Epoch 63/100\n",
      "1s - loss: 2.4266 - acc: 0.1310 - val_loss: 2.3937 - val_acc: 0.1569\n",
      "Epoch 64/100\n",
      "1s - loss: 2.4247 - acc: 0.1331 - val_loss: 2.3967 - val_acc: 0.1409\n",
      "Epoch 65/100\n",
      "1s - loss: 2.4224 - acc: 0.1356 - val_loss: 2.4066 - val_acc: 0.1408\n",
      "Epoch 66/100\n",
      "1s - loss: 2.4231 - acc: 0.1336 - val_loss: 2.3895 - val_acc: 0.1421\n",
      "Epoch 67/100\n",
      "1s - loss: 2.4221 - acc: 0.1349 - val_loss: 2.3891 - val_acc: 0.1541\n",
      "Epoch 68/100\n",
      "1s - loss: 2.4217 - acc: 0.1345 - val_loss: 2.3863 - val_acc: 0.1484\n",
      "Epoch 69/100\n",
      "1s - loss: 2.4264 - acc: 0.1317 - val_loss: 2.3989 - val_acc: 0.1447\n",
      "Epoch 70/100\n",
      "1s - loss: 2.4271 - acc: 0.1350 - val_loss: 2.3982 - val_acc: 0.1192\n",
      "Epoch 71/100\n",
      "1s - loss: 2.4298 - acc: 0.1324 - val_loss: 2.4060 - val_acc: 0.1206\n",
      "Epoch 72/100\n",
      "1s - loss: 2.4338 - acc: 0.1284 - val_loss: 2.3910 - val_acc: 0.1442\n",
      "Epoch 73/100\n",
      "1s - loss: 2.4286 - acc: 0.1303 - val_loss: 2.3959 - val_acc: 0.1410\n",
      "Epoch 74/100\n",
      "1s - loss: 2.4228 - acc: 0.1321 - val_loss: 2.3920 - val_acc: 0.1153\n",
      "Epoch 75/100\n",
      "1s - loss: 2.4225 - acc: 0.1335 - val_loss: 2.3867 - val_acc: 0.1532\n",
      "Epoch 76/100\n",
      "1s - loss: 2.4233 - acc: 0.1340 - val_loss: 2.3954 - val_acc: 0.1110\n",
      "Epoch 77/100\n",
      "1s - loss: 2.4217 - acc: 0.1347 - val_loss: 2.4010 - val_acc: 0.1357\n",
      "Epoch 78/100\n",
      "1s - loss: 2.4228 - acc: 0.1348 - val_loss: 2.3988 - val_acc: 0.1519\n",
      "Epoch 79/100\n",
      "1s - loss: 2.4225 - acc: 0.1343 - val_loss: 2.3934 - val_acc: 0.1552\n",
      "Epoch 80/100\n",
      "1s - loss: 2.4225 - acc: 0.1343 - val_loss: 2.3920 - val_acc: 0.1345\n",
      "Epoch 81/100\n",
      "1s - loss: 2.4226 - acc: 0.1329 - val_loss: 2.3927 - val_acc: 0.1257\n",
      "Epoch 82/100\n",
      "1s - loss: 2.4216 - acc: 0.1388 - val_loss: 2.3902 - val_acc: 0.1413\n",
      "Epoch 83/100\n",
      "1s - loss: 2.4230 - acc: 0.1356 - val_loss: 2.3961 - val_acc: 0.1464\n",
      "Epoch 84/100\n",
      "1s - loss: 2.4235 - acc: 0.1320 - val_loss: 2.3869 - val_acc: 0.1401\n",
      "Epoch 85/100\n",
      "1s - loss: 2.4241 - acc: 0.1335 - val_loss: 2.3940 - val_acc: 0.1149\n",
      "Epoch 86/100\n",
      "1s - loss: 2.4215 - acc: 0.1346 - val_loss: 2.3881 - val_acc: 0.1463\n",
      "Epoch 87/100\n",
      "1s - loss: 2.4193 - acc: 0.1390 - val_loss: 2.3885 - val_acc: 0.1500\n",
      "Epoch 88/100\n",
      "1s - loss: 2.4207 - acc: 0.1400 - val_loss: 2.4080 - val_acc: 0.1074\n",
      "Epoch 89/100\n",
      "1s - loss: 2.4205 - acc: 0.1356 - val_loss: 2.3945 - val_acc: 0.1468\n",
      "Epoch 90/100\n",
      "1s - loss: 2.4209 - acc: 0.1354 - val_loss: 2.3853 - val_acc: 0.1543\n",
      "Epoch 91/100\n",
      "1s - loss: 2.4220 - acc: 0.1340 - val_loss: 2.3898 - val_acc: 0.1423\n",
      "Epoch 92/100\n",
      "1s - loss: 2.4202 - acc: 0.1389 - val_loss: 2.3971 - val_acc: 0.1168\n",
      "Epoch 93/100\n",
      "1s - loss: 2.4232 - acc: 0.1385 - val_loss: 2.4013 - val_acc: 0.1476\n",
      "Epoch 94/100\n",
      "1s - loss: 2.4267 - acc: 0.1337 - val_loss: 2.4143 - val_acc: 0.1124\n",
      "Epoch 95/100\n",
      "1s - loss: 2.4277 - acc: 0.1347 - val_loss: 2.3989 - val_acc: 0.1487\n",
      "Epoch 96/100\n",
      "1s - loss: 2.4292 - acc: 0.1330 - val_loss: 2.3948 - val_acc: 0.1532\n",
      "Epoch 97/100\n",
      "1s - loss: 2.4299 - acc: 0.1299 - val_loss: 2.3918 - val_acc: 0.1538\n",
      "Epoch 98/100\n",
      "1s - loss: 2.4314 - acc: 0.1277 - val_loss: 2.4072 - val_acc: 0.0964\n",
      "Epoch 99/100\n",
      "1s - loss: 2.4325 - acc: 0.1249 - val_loss: 2.4063 - val_acc: 0.1586\n",
      "Epoch 100/100\n",
      "1s - loss: 2.4320 - acc: 0.1274 - val_loss: 2.3968 - val_acc: 0.1476\n",
      "NN trained in  2.206452417373657  minutes\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "model.fit(in_data.as_matrix(),true_classes_nn\n",
    "         , verbose=1,nb_epoch=100)\n",
    "print(\"NN trained in \", (time.time()-s)/60.0, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74432/74645 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(in_data.as_matrix(),true_classes_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.422510975863196, 0.13706209391297622]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111232/112071 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(112071, 12)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.predict_proba(test_sample.as_matrix())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module keras.models:\n",
      "\n",
      "fit(x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, **kwargs) method of keras.models.Sequential instance\n",
      "    Trains the model for a fixed number of epochs.\n",
      "    \n",
      "    # Arguments\n",
      "        x: input data, as a Numpy array or list of Numpy arrays\n",
      "            (if the model has multiple inputs).\n",
      "        y: labels, as a Numpy array.\n",
      "        batch_size: integer. Number of samples per gradient update.\n",
      "        nb_epoch: integer, the number of epochs to train the model.\n",
      "        verbose: 0 for no logging to stdout,\n",
      "            1 for progress bar logging, 2 for one log line per epoch.\n",
      "        callbacks: list of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_split: float (0. < x < 1).\n",
      "            Fraction of the data to use as held-out validation data.\n",
      "        validation_data: tuple (X, y) to be used as held-out\n",
      "            validation data. Will override validation_split.\n",
      "        shuffle: boolean or str (for 'batch').\n",
      "            Whether to shuffle the samples at each epoch.\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "        class_weight: dictionary mapping classes to a weight value,\n",
      "            used for scaling the loss function (during training only).\n",
      "        sample_weight: Numpy array of weights for\n",
      "            the training samples, used for scaling the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape (samples, sequence_length),\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            sample_weight_mode=\"temporal\" in compile().\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo: check multi-input NN with data from different stages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the submission\n",
    "First create a matrix for the predictions of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-3b550a4eb05e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# assign our probabilities to the prediction array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs_per_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((len(test.device_id),len(groups.index.values)))\n",
    "# assign our probabilities to the prediction array\n",
    "for i in range(0,prediction.shape[0]):\n",
    "    prediction[i]=probs_per_group.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define function that prepares the valid submission csv\n",
    "It uses the test dataset and the prediction matrix as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareOutput(test, pred, label='talkingData'):\n",
    "    '''\n",
    "    Writes an valid submission file from the prediction matrix.\n",
    "    The valid output must look like: \n",
    "    device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\n",
    "    (id, probailities)\n",
    "\n",
    "    Arguments:\n",
    "    test  - the DataFrame with the device_id's to be tested\n",
    "    pred  - is the prediction matrix with pred.shape = (len(test.device_id,len(unique groups))\n",
    "    label - prefix of the submission file\n",
    "    \n",
    "    Return:\n",
    "    The merged submission dataset is returned.\n",
    "    '''\n",
    "    p = pd.DataFrame(pred)\n",
    "    p.columns = labelEnc.inverse_transform(p.columns)\n",
    "    i = pd.DataFrame(test.device_id.values) \n",
    "    i.columns = ['device_id']\n",
    "    merged= pd.concat([i,p], axis=1)\n",
    "    merged.to_csv(solution_dir+label+'_submission.csv', index=False)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entriesPerClass_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "o = prepareOutput(test,prediction,'entriesPerClass')\n",
    "%ls predictions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>F23-</th>\n",
       "      <th>F24-26</th>\n",
       "      <th>F27-28</th>\n",
       "      <th>F29-32</th>\n",
       "      <th>F33-42</th>\n",
       "      <th>F43+</th>\n",
       "      <th>M22-</th>\n",
       "      <th>M23-26</th>\n",
       "      <th>M27-28</th>\n",
       "      <th>M29-31</th>\n",
       "      <th>M32-38</th>\n",
       "      <th>M39+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002079943728939269</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074499</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.114957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1547860181818787117</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074499</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.114957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             device_id      F23-    F24-26    F27-28  F29-32    F33-42  \\\n",
       "0  1002079943728939269  0.067654  0.056132  0.041771   0.062  0.074499   \n",
       "1 -1547860181818787117  0.067654  0.056132  0.041771   0.062  0.074499   \n",
       "\n",
       "       F43+      M22-    M23-26    M27-28    M29-31    M32-38      M39+  \n",
       "0  0.056186  0.100315  0.128676  0.072945  0.097917  0.126948  0.114957  \n",
       "1  0.056186  0.100315  0.128676  0.072945  0.097917  0.126948  0.114957  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This worked. The ouput can be submitted to kaggle."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
