{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.ensemble import (GradientBoostingClassifier\n",
    "                              , RandomForestClassifier\n",
    "                              , AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## The preamble handles a few imports.\n",
       "## It also loads common functions and dicts: \n",
       "\n",
       "## Bosch challenge specific:\n",
       "* **getMCC**(tp,tn,fp,fn)\n",
       "\n",
       "## ML/Analytics functions:\n",
       "* **compare_train_test**(clf, ds_train, label_train, ds_test, label_test, mva='MVA', bins=50, use_vote=None, log=False)\n",
       "* **plot_classifier_output**( pred_train, pred_test, y_train, y_test, multipagepdf=None, bins = None, normalised = True )\n",
       "* **plot_correlations**(data,label='', \\*\\*kwds)\n",
       "* **optimisePars**(mva, points, data , classes, fraction=0.7, score = 'log_loss', cvs=5)\n",
       "\n",
       "---\n",
       "\n",
       "## Various\n",
       "* **showUniques**(df)\n",
       "* **ensure_dir**(directory)\n",
       "* **printBumper**(text, c='=', n=-1)\n",
       "* **intersec**(d1, d2)\n",
       "* **union**(d1, d2)\n",
       "\n",
       "---\n",
       "\n",
       "## Color dictionaries:\n",
       "* **Tableau10**\n",
       "* **Tableau10_Light**\n",
       "* **Tableau10_Medium**\n",
       "* **Tableau_20**\n",
       "* **ColorBlind10**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"\"\"\n",
    "## The preamble handles a few imports.\n",
    "## It also loads common functions and dicts: \n",
    "\n",
    "## Bosch challenge specific:\n",
    "* **getMCC**(tp,tn,fp,fn)\n",
    "\n",
    "## ML/Analytics functions:\n",
    "* **compare_train_test**(clf, ds_train, label_train, ds_test, label_test, mva='MVA', bins=50, use_vote=None, log=False)\n",
    "* **plot_classifier_output**( pred_train, pred_test, y_train, y_test, multipagepdf=None, bins = None, normalised = True )\n",
    "* **plot_correlations**(data,label='', \\*\\*kwds)\n",
    "* **optimisePars**(mva, points, data , classes, fraction=0.7, score = 'log_loss', cvs=5)\n",
    "\n",
    "---\n",
    "\n",
    "## Various\n",
    "* **showUniques**(df)\n",
    "* **ensure_dir**(directory)\n",
    "* **printBumper**(text, c='=', n=-1)\n",
    "* **intersec**(d1, d2)\n",
    "* **union**(d1, d2)\n",
    "\n",
    "---\n",
    "\n",
    "## Color dictionaries:\n",
    "* **Tableau10**\n",
    "* **Tableau10_Light**\n",
    "* **Tableau10_Medium**\n",
    "* **Tableau_20**\n",
    "* **ColorBlind10**\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMCC(tp,tn,fp,fn):\n",
    "    \"\"\"\n",
    "    Calculates Matthews correlation coefficient based on confusion matrix. \n",
    "    \n",
    "    For model scoring the sklearn.metrics.matthews_corrcoef(y_true, y_pred) \n",
    "    can be used.\n",
    "    \n",
    "    Argument:\n",
    "    tp,tn - Number of true positive and true negative observations\n",
    "    fp,fn - Number of false positive and false negative observations\n",
    "    \"\"\"\n",
    "    num = (tp*tn)-(fp*fn)\n",
    "    den = sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "    return float(num)/float(den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../../pythonTools/plotting/plot.py\n",
    "#Function to plot histogram with errorbars\n",
    "def _hist_errorbars( data, xerrs=True, *args, **kwargs) :\n",
    "  \"\"\"\n",
    "  Plot a histogram with error bars. \n",
    "  Accepts any kwarg accepted by either numpy.histogram or pyplot.errorbar\n",
    "  \"\"\"\n",
    "  import numpy as np\n",
    "  import matplotlib.pyplot as plt\n",
    "  import inspect\n",
    "\n",
    "  # pop off normed kwarg, since we want to handle it specially\n",
    "  norm = False\n",
    "  if 'normed' in list(kwargs.keys()) :\n",
    "    norm = kwargs.pop('normed')\n",
    "\n",
    "  # retrieve the kwargs for numpy.histogram\n",
    "  histkwargs = {}\n",
    "  for key, value in kwargs.items() :\n",
    "    if key in inspect.getargspec(np.histogram).args :\n",
    "      histkwargs[key] = value\n",
    "\n",
    "  histvals, binedges = np.histogram( data, **histkwargs )\n",
    "  yerrs = np.sqrt(histvals)\n",
    "\n",
    "  if norm :\n",
    "    nevents = float(sum(histvals))\n",
    "    binwidth = (binedges[1]-binedges[0])\n",
    "    histvals = histvals/nevents/binwidth\n",
    "    yerrs = yerrs/nevents/binwidth\n",
    "\n",
    "  bincenters = (binedges[1:]+binedges[:-1])/2\n",
    "\n",
    "  if xerrs :\n",
    "    xerrs = (binedges[1]-binedges[0])/2\n",
    "  else :\n",
    "    xerrs = None\n",
    "\n",
    "  # retrieve the kwargs for errorbar\n",
    "  ebkwargs = {}\n",
    "  for key, value in kwargs.items() :\n",
    "    if key in inspect.getargspec(plt.errorbar).args :\n",
    "      ebkwargs[key] = value\n",
    "    if key == 'color':\n",
    "      ebkwargs['ecolor'] = value\n",
    "      ebkwargs['mfc'] = value\n",
    "    if key == 'label':\n",
    "      ebkwargs['label'] = value\n",
    "\n",
    "  out = plt.errorbar(bincenters, histvals, yerrs, xerrs, fmt=\"o\", mec='black', ms=8, **ebkwargs)\n",
    "\n",
    "\n",
    "  if 'log' in list(kwargs.keys()) :\n",
    "    if kwargs['log'] :\n",
    "      plt.yscale('log')\n",
    "\n",
    "  if 'range' in list(kwargs.keys()) :\n",
    "    plt.xlim(*kwargs['range'])\n",
    "\n",
    "  return out\n",
    "\n",
    "#Function to plot classifier output\n",
    "def plot_classifier_output( pred_train, pred_test, y_train, y_test,\n",
    "              multipagepdf=None, bins = None, normalised = True ):\n",
    "  \"\"\"\n",
    "  Plots classifier output. \n",
    "  Works nicely with yandex/rep folding classifiers\n",
    "  \n",
    "  Arguments:\n",
    "  pred_train: predictions for the trainings sample from the classifier.\n",
    "\n",
    "  pred_test: predictions for the test sample from the classifier.\n",
    "\n",
    "  y_train: true labels for the train sample. \n",
    "\n",
    "  y_test: true labels for the test sameple.\n",
    "\n",
    "  multipagepdf: argument to pass multipagepdf instance.\n",
    "\n",
    "  bins: binning for the resulting plot.\n",
    "\n",
    "  normalised: control whether the plots are drawn normalised.\n",
    "  \"\"\"\n",
    "  import numpy as np\n",
    "  import matplotlib.pyplot as plt\n",
    "  import inspect\n",
    "  #Set binning\n",
    "  binning = ( bins if bins is not None\n",
    "              else np.linspace(np.min(pred_train), np.max(pred_train), 51) )\n",
    "\n",
    "  #Set label to signal (=1) if label is None (for application )\n",
    "  if y_train is None:\n",
    "      y_train = np.ones(len(pred_train))\n",
    "  if y_test is None:\n",
    "      y_test = np.ones(len(pred_test))\n",
    "\n",
    "  #Plot training sample\n",
    "  plt.hist(pred_train[y_train<0.5], bins = binning, normed=normalised, histtype='stepfilled', color='b', alpha = 0.5,\n",
    "           linewidth=0, label=\"Training Background\")\n",
    "  plt.hist(pred_train[y_train>0.5], bins = binning, normed=normalised, color='r', alpha = 0.5,\n",
    "           linewidth=0, label=\"Training Signal\", histtype='stepfilled')\n",
    "\n",
    "  #Plot test sample\n",
    "  _hist_errorbars( pred_test[y_test<0.5], xerrs=True, bins = binning, normed=normalised,\n",
    "                  color='b', label=\"Test Background\")\n",
    "  _hist_errorbars( pred_test[y_test>0.5], xerrs=True, bins = binning, normed=normalised,\n",
    "                  color='r', label=\"Test Signal\")\n",
    "\n",
    "\n",
    "  #plt.title(\"Classifier Output - Signal vs. Background and Test vs Training\", fontsize=23)\n",
    "\n",
    "  plt.xlabel(\"Classifier Output\", fontsize=23)\n",
    "  plt.ylabel(\"Normalised Events\" if normalised else \"Events\", fontsize=23)\n",
    "\n",
    "  #Restrict plot to binning area\n",
    "  plt.xlim(binning[0], binning[-1])\n",
    "\n",
    "  #Plot legend\n",
    "  plt.legend(loc='best', fontsize=19)\n",
    "  #Save plot\n",
    "  #multipagepdf.savefig(bbox_inches = 'tight')\n",
    "  #plt.close()\n",
    "  #Plotted Classifier Output to {}\".format(options.plots))\n",
    "  return plt\n",
    "\n",
    "def compare_train_test(clf, ds_train, label_train, \n",
    "                      ds_test, label_test, mva='MVA', \n",
    "                      bins=50, use_vote=None, log=False):\n",
    "\n",
    "  \"\"\"\n",
    "  Plots output of the classifier for the training and test dataset.\n",
    "\n",
    "  Arguments:\n",
    "  clf: classifier object.\n",
    "\n",
    "  ds_train: dataset used for training.\n",
    "  \n",
    "  label_train: labels for the training datasets.\n",
    "  \n",
    "  ds_test: dataset used to test the classifier performance.\n",
    "  \n",
    "  label_test: labels of the corresponding test dataset.\n",
    "  \n",
    "  mva: Name of the ML method (used as axis label, str).\n",
    "  \n",
    "  bins: Number of bins (int).\n",
    "\n",
    "  use_vote: Function to be able to use vote functions for folding classifiers (CAUTION: is not strictly correct).\n",
    "\n",
    "  log: In order to use a logarithmic scale on the plots (boolean).\n",
    "  \"\"\"\n",
    "  import numpy as np\n",
    "  import matplotlib.pyplot as plt\n",
    "  import inspect\n",
    "  decisions = []\n",
    "  ns_train = len(ds_train[label_train>0.5])\n",
    "  nb_train = len(ds_train[label_train<0.5])\n",
    "\n",
    "  for X,y in ((ds_train, label_train), (ds_test, label_test)):\n",
    "    if use_vote == None:\n",
    "      d1 = clf.predict_proba(X[y>0.5])[:,1]#.ravel()\n",
    "      d2 = clf.predict_proba(X[y<0.5])[:,1]#.ravel()\n",
    "    else:\n",
    "      d1 = clf.predict_proba(X[y>0.5],vote_function=use_vote)[:,1]#.ravel()\n",
    "      d2 = clf.predict_proba(X[y<0.5],vote_function=use_vote)[:,1]#.ravel()\n",
    "\n",
    "    decisions += [d1, d2]\n",
    "      \n",
    "  low = min(np.min(d) for d in decisions)\n",
    "  high = max(np.max(d) for d in decisions)\n",
    "  low_high = (low,high)\n",
    "  _, ax = plt.subplots()\n",
    "  ys, bins, _ = plt.hist(decisions[0],\n",
    "           color='r', alpha=0.5, range=low_high, bins=bins,\n",
    "           histtype='step', normed=True,\n",
    "           label='S (train)',log=log)\n",
    "  yb, _, _ = plt.hist(decisions[1],\n",
    "           color='b', alpha=0.5, range=low_high, bins=bins,\n",
    "           histtype='step', normed=True,\n",
    "           label='B (train)',log=log)\n",
    "  width = (bins[1] - bins[0])\n",
    "  \n",
    "  minY = 1./float(max(ns_train,nb_train))/width\n",
    "  maxY = 6*max(yb.max(),ys.max())\n",
    "  \n",
    "  if log==True:\n",
    "    plt.ylim([minY,maxY])\n",
    "  hist, bins = np.histogram(decisions[2],\n",
    "                            bins=bins, range=low_high, normed=True)\n",
    "  \n",
    "  scale = len(decisions[2]) / sum(hist)\n",
    "  err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "  width = (bins[1] - bins[0])\n",
    "  center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "  fillx = [bins[0]]\n",
    "  fillys = [ys[0]]\n",
    "  fillyb = [yb[0]]\n",
    "  for i,(x,y_s,y_b) in enumerate(zip(bins,ys,yb)):\n",
    "    if i==0:\n",
    "      continue\n",
    "    if i==len(yb)-1:\n",
    "      fillx+=[x,x,bins[i+1]]\n",
    "      fillyb+=[yb[i-1],y_b,y_b]\n",
    "      fillys+=[ys[i-1],y_s,y_s]\n",
    "      continue\n",
    "    fillx+=[x,x]\n",
    "    fillys+=[ys[i-1],y_s]\n",
    "    fillyb+=[yb[i-1],y_b]\n",
    "\n",
    "  ax.fill_between(fillx,minY,fillys, color='r', alpha=0.5)\n",
    "  ax.fill_between(fillx,minY,fillyb, color='b', alpha=0.5)\n",
    "  \n",
    "  plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
    "\n",
    "  hist, bins = np.histogram(decisions[3],\n",
    "                            bins=bins, range=low_high, normed=True)\n",
    "  scale = len(decisions[3]) / sum(hist)\n",
    "  err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "  plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
    "\n",
    "  plt.xlabel(mva+\" output\")\n",
    "  plt.ylabel(\"Arbitrary units\")\n",
    "  plt.legend(loc='best')\n",
    "  return plt\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# ========================================\n",
    "\n",
    "def plot_correlations(data,label='', **kwds):\n",
    "    \"\"\"\n",
    "    Calculate pairwise correlation between features.\n",
    "    \n",
    "    Arguments:\n",
    "    data: Pandas.DataFrame on which the correlations are calculated.\n",
    "\n",
    "    label: prefix for the plot title and file name.\n",
    "\n",
    "    Extra arguments are passed on to DataFrame.corr()\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from pythonTools import ensure_dir\n",
    "    # simply call df.corr() to get a table of\n",
    "    # correlation values if you do not need\n",
    "    # the fancy plotting\n",
    "    corrmat = data.corr(**kwds)\n",
    "\n",
    "    fig, ax1 = plt.subplots(ncols=1, figsize=(10,9))\n",
    "    \n",
    "    opts = {'cmap': plt.get_cmap(\"RdBu\"),\n",
    "            'vmin': -1, 'vmax': +1}\n",
    "    heatmap1 = ax1.pcolor(corrmat, **opts)\n",
    "    fig.colorbar(heatmap1, ax=ax1)\n",
    "\n",
    "\n",
    "    ax1.set_title(label+\" Correlations_{}vars\".format(len(data.columns)-1))\n",
    "\n",
    "    labels = corrmat.columns.values\n",
    "    for ax in (ax1,):\n",
    "        # shift location of ticks to center of the bins\n",
    "        ax.set_xticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_yticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_xticklabels(labels, fontsize=12,minor=False, ha='right', rotation=70)\n",
    "        ax.set_yticklabels(labels, fontsize=12,minor=False)\n",
    "        \n",
    "    fig.set_tight_layout(True)\n",
    "    dir = 'plots'\n",
    "    ensure_dir(dir)\n",
    "    fig.savefig(dir+'/'+label+'_correlation_{}vars.pdf'.format(len(data.columns)))\n",
    "    fig.savefig(dir+'/'+label+'_correlation_{}vars.png'.format(len(data.columns)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../../pythonTools/utilities.py\n",
    "'''\n",
    "Module containing small general helper functions\n",
    "'''\n",
    "\n",
    "def showUniques(df):\n",
    "  \"\"\"\n",
    "  Helper function that shows unique entries per DataFrame column.\n",
    "\n",
    "  Arguments:\n",
    "  df: the pandas DataFrame in question.\n",
    "  \"\"\"\n",
    "  \n",
    "  import pandas as pd\n",
    "  from pandas import DataFrame \n",
    "\n",
    "  print(\"Number of rows: \", len(df))\n",
    "  print(\"Number of unique values per column: \")\n",
    "  for col in df.columns:\n",
    "    print(\"Column {}: \".format(col), df[col].nunique())\n",
    "\n",
    "#====================================\n",
    "#====================================\n",
    "\n",
    "\n",
    "def ensure_dir(directory):\n",
    "  \"\"\"When directory is not present, create it.\n",
    "\n",
    "  Arguments: \n",
    "  directory: name of directory.\n",
    "  \"\"\"\n",
    "  import os\n",
    "  if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "#====================================\n",
    "#====================================\n",
    "\n",
    "def printBumper(text, c='=', n=-1):\n",
    "  \"\"\"Print a text with separating character. \n",
    "\n",
    "  Arguments: \n",
    "  text: text to be printed between separating character.\n",
    "\n",
    "  c: separating character. \n",
    "\n",
    "  n: number of printed characters prior to the text (default -1: print as many characters as length of text).\"\"\"\n",
    "  if n==-1:\n",
    "    times = len(text)\n",
    "  else:\n",
    "    times = n\n",
    "  print(c*times)\n",
    "  print(text, ( (times-len(text)-1)*c ))\n",
    "  print(c*times)\n",
    "\n",
    "#====================================\n",
    "#====================================\n",
    "\n",
    "def intersec(d1, d2):\n",
    "  \"\"\"\n",
    "  Returns list of intersecting elements.\n",
    "\n",
    "  Arguments:\n",
    "  d1/d2 - Objects that can be 'casted' as a sets.\n",
    "  \"\"\"\n",
    "  return list(set(d1).intersection(set(d2)))\n",
    "\n",
    "def union(d1,d2):\n",
    "  \"\"\"\n",
    "  Returns list of union elements.\n",
    "\n",
    "  Arguments:\n",
    "  d1/d2 - Objects that can be 'casted' as a sets.\n",
    "  \"\"\"\n",
    "  return list(set(d1).union(set(d2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../../pythonTools/ml_utilities.py\n",
    "'''\n",
    "Contains useful machine learning functions.\n",
    "'''\n",
    "\n",
    "def optimisePars(mva, points, data , classes, fraction=0.7, **kwargs):\n",
    "    '''\n",
    "    Funtion to optimise hyper-parameters. Follows sklearn example:\n",
    "    \"example-model-selection-grid-search-digits-py\"\n",
    "    \n",
    "    Arguments:\n",
    "    mva - multivariate method to optimise\n",
    "    points - dictionary that holds optimisation points (hyper-parameters)\n",
    "    data - your training data\n",
    "    classes - true categories/classes/labels \n",
    "    fraction - fraction of training/test split\n",
    "    **kwargs:\n",
    "    scoring - score function to optimise the classifier (eg 'log_loss')\n",
    "    cv - number of cross-validation folds or cross-validation generator\n",
    "    n_jobs - number of parallel jobs for grid-search\n",
    "    verbose - verbosity\n",
    "\n",
    "    Returns:\n",
    "    clf - GridSearchCV classifier.\n",
    "\n",
    "    To-Do:\n",
    "    - classification report does not exactly work every time. \n",
    "    '''\n",
    "    import time\n",
    "    # Tuning hyper-parameters for log_loss score\n",
    "    \n",
    "    # Splits data\n",
    "    data_train, data_test, classes_train, classes_test =  train_test_split(\n",
    "    data, classes, test_size=fraction, random_state=0)\n",
    "    s =  time.time()\n",
    "    clf = GridSearchCV(mva, points, cv=cvs,\n",
    "                       scoring=score, n_jobs=njobs, \n",
    "                       verbose=vbs)\n",
    "                       \n",
    "    clf.fit(data_train, classes_train)\n",
    "\n",
    "    print('GridSearch completed after ', (time.time()-s)/60.0, ' minutes.')\n",
    "    print()\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "      print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = classes_test, clf.predict_proba(data_test)\n",
    "    print(\"Log loss score on test sample: \", log_loss(y_true, y_pred))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ../../pythonTools/plotting/nice_colors.py\n",
    "'''\n",
    "Color dictionaries from \n",
    "http://tableaufriction.blogspot.de/2012/11/finally-you-can-use-tableau-data-colors.html\n",
    "'''\n",
    "\n",
    "Tableau10 = { \n",
    "\t'blue'   : (31/255.,119/255.,180/255.),\n",
    "\t'orange' : (255/255.,127/255.,14/255.),\n",
    "\t'green'  : (44/255.,160/255.,44/255.),\n",
    "\t'red'    : (214/255.,39/255.,40/255.),\n",
    "\t'purple' : (148/255.,103/255.,189/255.),\n",
    "\t'brown'  : (140/255.,86/255.,75/255.),\n",
    "\t'pink'   : (227/255.,119/255.,194/255.),\n",
    "\t'gray'   : (127/255.,127/255.,127/255.),\n",
    "\t'olive'  : (188/255.,189/255.,34/255.),\n",
    "\t'cyan'   : (23/255.,190/255.,207/255.)\n",
    "}\n",
    "\n",
    "Tableau10_Light\t= {\n",
    "\t'blue'   : (174/255.,199/255.,232/255.),\n",
    "\t'orange' : (255/255.,187/255.,120/255.),\n",
    "\t'green'  : (152/255.,223/255.,138/255.),\n",
    "\t'red'    : (255/255.,152/255.,150/255.),\n",
    "\t'purple' : (197/255.,176/255.,213/255.),\n",
    "\t'brown'  : (196/255.,156/255.,148/255.),\n",
    "\t'pink'   : (247/255.,182/255.,210/255.),\n",
    "\t'gray'   : (199/255.,199/255.,199/255.),\n",
    "\t'olive'  : (219/255.,219/255.,141/255.),\n",
    "\t'cyan'   : (158/255.,218/255.,229/255.)\n",
    "}\n",
    "\n",
    "Tableau10_Medium = { \n",
    "\t\"blue\"   : (114/255.,158/255.,206/255.),\n",
    "\t\"orange\" : (255/255.,158/255.,74/255.),\n",
    "\t\"green\"  : (103/255.,191/255.,92/255.),\n",
    "\t\"red\"    : (237/255.,102/255.,93/255.),\n",
    "\t\"purple\" : (173/255.,139/255.,201/255.),\n",
    "\t\"brown\"  : (168/255.,120/255.,110/255.),\n",
    "\t\"pink\"   : (237/255.,151/255.,202/255.),\n",
    "\t\"gray\"   : (162/255.,162/255.,162/255.),\n",
    "\t\"olive\"  : (205/255.,204/255.,93/255.),\n",
    "\t\"cyan\"   : (109/255.,204/255.,218/255.)\n",
    "}\n",
    "\n",
    "Tableau_20 = {\n",
    "\t\"orange_light\"    : (255/255.,187/255.,120/255.),\n",
    "\t\"orange\"          : (255./255.,127./255.,14/255.),\n",
    "\t\"blue_light\"      : (174./255.,199./255.,232/255.),\n",
    "\t\"blue\"            : (31./255.,119./255.,180/255.),\n",
    "\t\"green_light\"     : (152./255.,223./255.,138/255.),\n",
    "\t\"green\"           : (44./255.,160./255.,44/255.),\n",
    "\t\"red_light\"       : (255./255.,152./255.,150/255.),\n",
    "\t\"red\"             : (214./255.,39./255.,40/255.),\n",
    "\t\"purple_light\"    : (197./255.,176./255.,213/255.),\n",
    "\t\"purple\"          : (148./255.,103./255.,189/255.),\n",
    "\t\"pink_light\"      : (247./255,182./255.,210/255.),\n",
    "\t\"pink\"            : (227./255,119./255.,194/255.),\n",
    "\t\"brown_light\"     : (196./255,156./255.,148/255.),\n",
    "\t\"brown\"           : (140./255,86./255.,75/255.),\n",
    "\t\"grey\"            : (127./255,127./255.,127/255.),\n",
    "\t\"grey_light\"      : (199./255,199./255.,199/255.),\n",
    "\t\"lime_light\"      : (219./255,219./255.,141/255.),\n",
    "\t\"lime\"            : (188./255,189./255.,34/255.),\n",
    "\t\"sapphire_light\"  : (158./255,218./255.,229/255.),\n",
    "\t\"sapphire\"        : (23./255,190./255.,207/255.)\n",
    "}\n",
    "\n",
    "ColorBlind10 = {\n",
    "\t\"azure\"        : (0./255.,107./255.,164/255.),\n",
    "\t\"orange\"       : (255./255.,128./255.,14/255.),\n",
    "\t\"grey_medium\"  : (171./255.,171./255.,171/255.),\n",
    "\t\"grey_dark\"    : (89./255.,89./255.,89/255.),\n",
    "\t\"blue_medium\"  : (95./255.,158./255.,209/255.),\n",
    "\t\"orange_dark\"  : (200./255.,82./255.,0/255.),\n",
    "\t\"grey\"         : (137./255.,137./255.,137/255.),\n",
    "\t\"blue_light\"   : (162./255.,200./255.,236/255.),\n",
    "\t\"orange_light\" : (255./255.,188./255.,121/255.),\n",
    "\t\"grey_light\"   : (207./255.,207./255.,207/255.)\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupnote]",
   "language": "python",
   "name": "conda-env-jupnote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
