{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Trained'  models\n",
    "* The first reference will be a model without any usage of knowledge. The probability to end up in the class for a test customer is assumed to be *n(customers per class)/all(customers)*.\n",
    "* The second model is a multi-class boosted decision tree classifier\n",
    "* The third model uses a boosted decision tree to classify male or female and a regression model to predict the age.\n",
    "\n",
    "---\n",
    "\n",
    "Using logistic loss evaluation the prediction scores:\n",
    "* Naive entries per class model: \n",
    "   - loss: **2.42786222642**\n",
    "   - score on kaggle: **2.42762**\n",
    "* AdaBoost Classifier with: \n",
    "   - algorithm='SAMME.R'\n",
    "   - DT(max_features=4, min_samples_leaf=74.645) \n",
    "   - learning_rate=0.15 \n",
    "   - n_estimators=800 \n",
    "   - random_state=666\n",
    "   - _loss on training_: **2.39243**\n",
    "   - _with k-Folding, mean loss_: **--**\n",
    "   - _separate optimised BDTs for nEvts==0 and nEvts>=1_: **--**\n",
    "* Gradient Boosting:\n",
    "   - loss: deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/mschlupp/pythonTools\")\n",
    "tmp = %pwd\n",
    "files_dir = tmp + \"/files/\" \n",
    "solution_dir = tmp+'/predictions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv(files_dir+'gender_age_train.csv')\n",
    "#test = pd.read_csv(files_dir+'gender_age_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_events.csv        phone_brand_device_model.csv\r\n",
      "app_labels.csv        phone_brand_device_model_engl.csv\r\n",
      "events.csv            sample_submission.csv\r\n",
      "events_day_hour.csv   traintest_fullevt.csv\r\n",
      "gender_age_test.csv   traintest_phone.csv\r\n",
      "gender_age_train.csv  traintest_phone_day_hour.csv\r\n",
      "label_categories.csv  traintest_phone_evts.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls files/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us first remind ourself, what we have in our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_set = pd.read_csv(files_dir+'traintest_fullevt.csv', nrows=0) # just read the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'device_id', 'gender', 'group', 'isTrain', 'phone_brand',\n",
      "       'device_model', 'hasEvents', 'nEvts', 'longitude_mean',\n",
      "       'longitude_variance', 'latitude_mean', 'latitude_variance',\n",
      "       'usageTime_mean', 'usageTime_variance', 'usageDay_mean',\n",
      "       'usageDay_variance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols=new_set.columns\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = cols.drop('hasEvents','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's read the data chunkwise and split in train and test sample\n",
    "# we only want the training sample for now.\n",
    "iter_csv = pd.read_csv(files_dir+'traintest_fullevt.csv', usecols=cols, iterator=True, chunksize=1500)\n",
    "train = pd.concat([chunk[chunk['isTrain'] ==1] for chunk in iter_csv])\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>device_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>group</th>\n",
       "      <th>isTrain</th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>nEvts</th>\n",
       "      <th>longitude_mean</th>\n",
       "      <th>longitude_variance</th>\n",
       "      <th>latitude_mean</th>\n",
       "      <th>latitude_variance</th>\n",
       "      <th>usageTime_mean</th>\n",
       "      <th>usageTime_variance</th>\n",
       "      <th>usageDay_mean</th>\n",
       "      <th>usageDay_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>-8076087639492063270</td>\n",
       "      <td>M</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>1</td>\n",
       "      <td>小米</td>\n",
       "      <td>MI 2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>-2897161552818060146</td>\n",
       "      <td>M</td>\n",
       "      <td>M32-38</td>\n",
       "      <td>1</td>\n",
       "      <td>小米</td>\n",
       "      <td>MI 2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age            device_id gender   group  isTrain phone_brand device_model  \\\n",
       "0   35 -8076087639492063270      M  M32-38        1          小米         MI 2   \n",
       "1   35 -2897161552818060146      M  M32-38        1          小米         MI 2   \n",
       "\n",
       "   nEvts  longitude_mean  longitude_variance  latitude_mean  \\\n",
       "0      0              -1                  -1             -1   \n",
       "1      0              -1                  -1             -1   \n",
       "\n",
       "   latitude_variance  usageTime_mean  usageTime_variance  usageDay_mean  \\\n",
       "0                 -1              -1                  -1             -1   \n",
       "1                 -1              -1                  -1             -1   \n",
       "\n",
       "   usageDay_variance  \n",
       "0                 -1  \n",
       "1                 -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference  model\n",
    "The first reference will be a model without any usage of knowledge.\n",
    "The probability to end up in the class for a test customer is assumed to be *n(customers per class)/all(customers)*.\n",
    "\n",
    "---\n",
    "\n",
    "Using logistic loss evaluation the prediction scores:\n",
    "* Naive entries per class model: \n",
    "   - loss: **2.42786222642**\n",
    "   - score on kaggle: **2.42762**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = train.groupby('group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups.device_id = groups.device_id/len(train.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "F23-      0.067654\n",
       "F24-26    0.056132\n",
       "F27-28    0.041771\n",
       "F29-32    0.062000\n",
       "F33-42    0.074499\n",
       "F43+      0.056186\n",
       "M22-      0.100315\n",
       "M23-26    0.128676\n",
       "M27-28    0.072945\n",
       "M29-31    0.097917\n",
       "M32-38    0.126948\n",
       "M39+      0.114957\n",
       "Name: device_id, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.device_id # show our naive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the prediction matrix\n",
    "prediction = np.zeros((len(train.age),len(groups.device_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us use the log_loss \n",
    "# (sklearn's logistic loss / cross-entropy) \n",
    "# implementation to score our prediction\n",
    "\n",
    "# first transform group into numerical classes\n",
    "labelEnc = LabelEncoder()\n",
    "labelEnc.fit(train.group)\n",
    "true_group = labelEnc.transform(train.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg = df(columns=groups.index.values)\n",
    "probs_per_group = dg.append(groups.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign our probabilities to the prediction array\n",
    "for i in range(0,prediction.shape[0]):\n",
    "    prediction[i]=probs_per_group.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss of our prediction is: \n",
      "2.42786222642\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic loss of our prediction is: \")\n",
    "print(log_loss(true_group,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class boosted decision tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold, LabelKFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our multi-class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes = pd.DataFrame(np.zeros((len(train.device_id),len(train.group.unique()))))\n",
    "true_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F23-', 'F24-26', 'F27-28', 'F29-32', 'F33-42', 'F43+', 'M22-',\n",
       "       'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+'], dtype=object)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_groups = LabelEncoder()\n",
    "le_groups.fit(train.group.unique())\n",
    "le_groups.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_classes.columns = le_groups.inverse_transform(list(true_classes.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F23-', 'F24-26', 'F27-28', 'F29-32', 'F33-42', 'F43+', 'M22-',\n",
       "       'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_classes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still allive...  0\n",
      "still allive...  10001\n",
      "still allive...  20002\n",
      "still allive...  30003\n",
      "still allive...  40004\n",
      "still allive...  50005\n",
      "still allive...  60006\n",
      "still allive...  70007\n"
     ]
    }
   ],
   "source": [
    "# There should be a smarter way. It's late, sorry --> sparse matrices\n",
    "for i,row,x in zip(range(0,len(train.group)),true_classes.iterrows(), train.group):\n",
    "    if i % 10001 == 0: \n",
    "        print('still allive... ', i)\n",
    "    row[1][x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_classes.columns = le_groups.transform(true_classes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_brand</th>\n",
       "      <th>device_model</th>\n",
       "      <th>nEvts</th>\n",
       "      <th>longitude_mean</th>\n",
       "      <th>longitude_variance</th>\n",
       "      <th>latitude_mean</th>\n",
       "      <th>latitude_variance</th>\n",
       "      <th>usageTime_mean</th>\n",
       "      <th>usageTime_variance</th>\n",
       "      <th>usageDay_mean</th>\n",
       "      <th>usageDay_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phone_brand  device_model  nEvts  longitude_mean  longitude_variance  \\\n",
       "0           47           677      0              -1                  -1   \n",
       "\n",
       "   latitude_mean  latitude_variance  usageTime_mean  usageTime_variance  \\\n",
       "0             -1                 -1              -1                  -1   \n",
       "\n",
       "   usageDay_mean  usageDay_variance  \n",
       "0             -1                 -1  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the dataset we are actually using for training\n",
    "in_data = train.drop(['age','gender','group','device_id','isTrain'], axis=1)\n",
    "\n",
    "# now transform string variables into numerical categories \n",
    "le_phone = LabelEncoder()\n",
    "le_device = LabelEncoder()\n",
    "le_phone.fit(in_data['phone_brand'].unique())\n",
    "le_device.fit(in_data['device_model'].unique())\n",
    "\n",
    "in_data['device_model'] = le_device.transform(in_data['device_model'])\n",
    "in_data['phone_brand'] = le_phone.transform(in_data['phone_brand'])\n",
    "\n",
    "# let's check our dataset\n",
    "in_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10, 10, ...,  6, 10,  7])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_class = le_groups.transform(train.group)\n",
    "true_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no hyper-parameter tuning, just from experience...\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(min_samples_leaf=0.01*len(true_class.index)),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         learning_rate=0.05,\n",
    "                         n_estimators=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=4, max_leaf_nodes=None, min_samples_leaf=74.645,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.15, n_estimators=800, random_state=666)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "s=time.time()\n",
    "bdt.fit(in_data,true_class)\n",
    "print('training finished after: ', (time.time()-s)/60.0, ' minutes.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration of predictions:  0.8685107827186584  minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.08271853,  0.08349847,  0.08299848, ...,  0.083619  ,\n",
       "         0.08381157,  0.08353482],\n",
       "       [ 0.08271853,  0.08349847,  0.08299848, ...,  0.083619  ,\n",
       "         0.08381157,  0.08353482],\n",
       "       [ 0.07350266,  0.0755404 ,  0.06335669, ...,  0.0959348 ,\n",
       "         0.10704796,  0.09117485],\n",
       "       ..., \n",
       "       [ 0.08319621,  0.08313595,  0.08278658, ...,  0.0834848 ,\n",
       "         0.08365674,  0.08358209],\n",
       "       [ 0.07508997,  0.04637466,  0.04418379, ...,  0.09102956,\n",
       "         0.11005088,  0.11972742],\n",
       "       [ 0.0914771 ,  0.04563885,  0.04851729, ...,  0.10231873,\n",
       "         0.09931683,  0.1022712 ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=time.time()\n",
    "probas = bdt.predict_proba(in_data)\n",
    "print('duration of predictions: ', (time.time()-s)/60.0, ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3924290904229317"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(true_classes,probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really overwhelming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### does not make much sense any more.\n",
    "mean_loss = 0\n",
    "clfs = []\n",
    "for indx_train, indx_test in kf:\n",
    "    s = time.time()\n",
    "    data_train, data_test = in_data.iloc[indx_train], in_data.iloc[indx_test]\n",
    "    class_train, class_test = true_class[indx_train], true_class[indx_test]\n",
    "    bdt.fit(data_train,class_train)\n",
    "    probs = bdt.predict_proba(data_test)\n",
    "    loss = log_loss(class_test,probs)\n",
    "    mean_loss+=loss\n",
    "    clfs.append(bdt)\n",
    "    print('fold yields log loss fundction value of: ', loss)\n",
    "    print('fold trained in: ', (time.time()-s)/60., ' minutes.')\n",
    "mean_loss/=len(kf)\n",
    "print('average loss of BDT model with 10 folds: ', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current data set up, two very different prediction scenarios are presend: eithe the data has event entries, or it doesn't. We'll test if two optimised algorithms for each scenario help to reduce the loss score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=4, max_leaf_nodes=None, min_samples_leaf=74.645,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'),\n",
       " 'base_estimator__class_weight': None,\n",
       " 'base_estimator__criterion': 'gini',\n",
       " 'base_estimator__max_depth': None,\n",
       " 'base_estimator__max_features': 4,\n",
       " 'base_estimator__max_leaf_nodes': None,\n",
       " 'base_estimator__min_samples_leaf': 74.645,\n",
       " 'base_estimator__min_samples_split': 2,\n",
       " 'base_estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'base_estimator__presort': False,\n",
       " 'base_estimator__random_state': None,\n",
       " 'base_estimator__splitter': 'best',\n",
       " 'learning_rate': 0.15,\n",
       " 'n_estimators': 800,\n",
       " 'random_state': 666}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainedModels/ad_hoc_BDT.pkl']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bdt, 'trainedModels/ad_hoc_BDT.pkl',compress=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimisePars(mva, points, data , classes, fraction=0.6, score = 'log_loss', cvs=5):\n",
    "    import time\n",
    "    print(\"# Tuning hyper-parameters for log_loss score\")\n",
    "    print()\n",
    "    \n",
    "    # Splits data\n",
    "    data_train, data_test, classes_train, classes_test =  train_test_split(\n",
    "    data, classes, test_size=fraction, random_state=0)\n",
    "    s =  time.time()\n",
    "    clf = GridSearchCV(mva, points, cv=cvs,\n",
    "                       scoring=score, n_jobs=4)\n",
    "                       \n",
    "    clf.fit(data_train, classes_train)\n",
    "\n",
    "    print('GridSearch completed after ', (time.time()-s)/60.0, ' minutes.')\n",
    "    print()\n",
    "    print(\"Best parameters set found on training set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on training set:\")\n",
    "    print()\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full training set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict_proba(data_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimise BDT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of points considered in the optimisation\n",
    "bdt_pars = {'learning_rate': [0.15,0.05,0.5],\n",
    " 'n_estimators': [300,800,500],\n",
    " 'base_estimator__min_samples_leaf': [10,50,100,500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_data['group'] = train.group\n",
    "data_hasEvts = in_data.query('nEvts>0')\n",
    "class_hasEvts =  le_groups.transform(data_hasEvts.group)\n",
    "data_noEvts = in_data.query('nEvts==0').drop(['nEvts','longitude_mean', 'longitude_variance',\n",
    "       'latitude_mean', 'latitude_variance', 'usageTime_mean',\n",
    "       'usageTime_variance', 'usageDay_mean', 'usageDay_variance'],axis=1)\n",
    "class_noEvts =  le_groups.transform(data_noEvts.group)\n",
    "\n",
    "data_hasEvts = data_hasEvts.drop(['group'],axis=1);\n",
    "data_noEvts = data_noEvts.drop(['group'],axis=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tune new models\n",
    "bdt_noEvts = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\")\n",
    "\n",
    "\n",
    "bdt_hasEvts = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm=\"SAMME.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for log_loss score\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimisePars(bdt_noEvts, bdt_pars, data_noEvts, class_noEvts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimisePars(bdt_hasEvts, bdt_pars, data_hasEvts, class_hasEvts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try GradientBoosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary holding omtimisation points\n",
    "GBpars = {'n_estimators': [100,300,400,700],\n",
    "          'min_samples_leaf' : [10, 50, 100, 500],\n",
    "           'learning_rate' : [0.4, 0.05, 0.1, 0.2]\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the submission\n",
    "First create a matrix for the predictions of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-3b550a4eb05e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# assign our probabilities to the prediction array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs_per_group\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros((len(test.device_id),len(groups.index.values)))\n",
    "# assign our probabilities to the prediction array\n",
    "for i in range(0,prediction.shape[0]):\n",
    "    prediction[i]=probs_per_group.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define function that prepares the valid submission csv\n",
    "It uses the test dataset and the prediction matrix as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareOutput(test, pred, label='talkingData'):\n",
    "    '''\n",
    "    Writes an valid submission file from the prediction matrix.\n",
    "    The valid output must look like: \n",
    "    device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\n",
    "    (id, probailities)\n",
    "\n",
    "    Arguments:\n",
    "    test  - the DataFrame with the device_id's to be tested\n",
    "    pred  - is the prediction matrix with pred.shape = (len(test.device_id,len(unique groups))\n",
    "    label - prefix of the submission file\n",
    "    \n",
    "    Return:\n",
    "    The merged submission dataset is returned.\n",
    "    '''\n",
    "    p = pd.DataFrame(pred)\n",
    "    p.columns = labelEnc.inverse_transform(p.columns)\n",
    "    i = pd.DataFrame(test.device_id.values) \n",
    "    i.columns = ['device_id']\n",
    "    merged= pd.concat([i,p], axis=1)\n",
    "    merged.to_csv(solution_dir+label+'_submission.csv', index=False)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entriesPerClass_submission.csv\r\n"
     ]
    }
   ],
   "source": [
    "o = prepareOutput(test,prediction,'entriesPerClass')\n",
    "%ls predictions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>F23-</th>\n",
       "      <th>F24-26</th>\n",
       "      <th>F27-28</th>\n",
       "      <th>F29-32</th>\n",
       "      <th>F33-42</th>\n",
       "      <th>F43+</th>\n",
       "      <th>M22-</th>\n",
       "      <th>M23-26</th>\n",
       "      <th>M27-28</th>\n",
       "      <th>M29-31</th>\n",
       "      <th>M32-38</th>\n",
       "      <th>M39+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002079943728939269</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074499</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.114957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1547860181818787117</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.041771</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074499</td>\n",
       "      <td>0.056186</td>\n",
       "      <td>0.100315</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.126948</td>\n",
       "      <td>0.114957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             device_id      F23-    F24-26    F27-28  F29-32    F33-42  \\\n",
       "0  1002079943728939269  0.067654  0.056132  0.041771   0.062  0.074499   \n",
       "1 -1547860181818787117  0.067654  0.056132  0.041771   0.062  0.074499   \n",
       "\n",
       "       F43+      M22-    M23-26    M27-28    M29-31    M32-38      M39+  \n",
       "0  0.056186  0.100315  0.128676  0.072945  0.097917  0.126948  0.114957  \n",
       "1  0.056186  0.100315  0.128676  0.072945  0.097917  0.126948  0.114957  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This worked. The ouput can be submitted to kaggle."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
